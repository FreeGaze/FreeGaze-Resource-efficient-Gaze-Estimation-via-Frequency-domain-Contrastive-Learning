{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af0d2845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 17:31:39.241844: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import random\n",
    "import copy\n",
    "import packages.baseline_model as baselibe_model\n",
    "import packages.common_functions as common_functions\n",
    "from skimage import io\n",
    "import cv2\n",
    "import packages.trans_in_rgb as trans_in_rgb\n",
    "import matplotlib.pyplot as plt\n",
    "import packages.CL_model as CL_model\n",
    "from jpeg2dct.numpy import load, loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca5bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGBProjectionHead(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    projection head for contrastive learning\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(512, bias_initializer=tf.keras.initializers.constant(0.01),\n",
    "                                            kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                                            bias_regularizer=regularizers.l2(1e-4))\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.dense2 = tf.keras.layers.Dense(256, bias_initializer=tf.keras.initializers.constant(0.01),\n",
    "                                            kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                                            bias_regularizer=regularizers.l2(1e-4))\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, input, training=None):\n",
    "        x = self.dense1(input)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        output = self.dense2(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a57bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet18_BaseEncoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, layer_params=None, method=\"late_concate\"):\n",
    "        super(Resnet18_BaseEncoder, self).__init__()\n",
    "\n",
    "        self.method = method\n",
    "        if layer_params is None:\n",
    "            layer_params = [2, 2, 2, 2]\n",
    "        self.y_input_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.cbcr_input_bn = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.layer1_y = make_basic_block_layer(filter_num=64,\n",
    "                                             blocks=1, dimen_match=True) # y:56,56,64\n",
    "        self.cb2_y = make_basic_block_layer(filter_num=128,\n",
    "                                             blocks=1, stride=2) # y:28,28,128\n",
    "        self.cb2_cbcr = tf.keras.layers.Conv2D(filters=128,\n",
    "                                               kernel_size=(1,1),\n",
    "                                               strides=1,\n",
    "                                               padding=\"same\")\n",
    "        self.cb_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.layer4 = make_basic_block_layer(filter_num=512,\n",
    "                                             blocks=2, # 14,14,512\n",
    "                                             stride=2)\n",
    "        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        y = self.y_input_bn(inputs[0], training=training)\n",
    "        cb_cr = self.cbcr_input_bn(inputs[1], training=training)\n",
    "\n",
    "        y = self.layer1_y(y, training=training) # 56,56,64\n",
    "        y = self.cb2_y(y, training=training) # 28,28,128\n",
    "        cb_cr = tf.nn.relu(self.cb_bn(self.cb2_cbcr(cb_cr), training=training)) #28,28,128\n",
    "        x = tf.concat((y, cb_cr), axis=3) #28,28,256\n",
    "        x = self.layer4(x, training=training) #14,14,512\n",
    "        x = self.avgpool(x)\n",
    "        output = self.flatten(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def make_basic_block_layer(filter_num, blocks, k_size=(3,3), stride=1, dimen_match=False):\n",
    "    res_block = tf.keras.Sequential()\n",
    "    res_block.add(BasicBlock(filter_num, k_size=k_size, stride=stride, dimen_match=dimen_match))\n",
    "\n",
    "    for _ in range(1, blocks):\n",
    "        res_block.add(BasicBlock(filter_num, stride=1))\n",
    "\n",
    "    return res_block\n",
    "\n",
    "\n",
    "class BasicBlock(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, filter_num, k_size=(3,3), stride=1, dimen_match=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                            kernel_size=k_size,\n",
    "                                            strides=stride,\n",
    "                                            padding=\"same\")\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                            kernel_size=k_size,\n",
    "                                            strides=1,\n",
    "                                            padding=\"same\")\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        if stride != 1 or dimen_match is True:\n",
    "            self.downsample = tf.keras.Sequential()\n",
    "            self.downsample.add(tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                                       kernel_size=(1, 1),\n",
    "                                                       strides=stride))\n",
    "            self.downsample.add(tf.keras.layers.BatchNormalization())\n",
    "        else:\n",
    "            self.downsample = lambda x: x\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        residual = self.downsample(inputs)\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "\n",
    "        output = tf.nn.relu(tf.keras.layers.add([residual, x]))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c65d2cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 17:31:44.281020: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-30 17:31:44.282921: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-30 17:31:44.326797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-30 17:31:44.327465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:05.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-04-30 17:31:44.327490: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-30 17:31:44.329486: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-04-30 17:31:44.329570: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-04-30 17:31:44.331606: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-30 17:31:44.331978: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-30 17:31:44.333919: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-04-30 17:31:44.335031: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-04-30 17:31:44.339093: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-04-30 17:31:44.339266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-30 17:31:44.339940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-30 17:31:44.340496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-04-30 17:31:44.341128: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-30 17:31:44.344416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-30 17:31:44.345023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:05.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-04-30 17:31:44.345046: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-30 17:31:44.345074: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-04-30 17:31:44.345084: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-04-30 17:31:44.345094: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-30 17:31:44.345114: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-30 17:31:44.345126: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-04-30 17:31:44.345137: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-04-30 17:31:44.345147: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-04-30 17:31:44.345209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-30 17:31:44.345811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-30 17:31:44.346373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-04-30 17:31:44.346414: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-30 17:31:44.983076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-30 17:31:44.983125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-04-30 17:31:44.983130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-04-30 17:31:44.983366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-30 17:31:44.984000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-30 17:31:44.984599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-30 17:31:44.985154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10073 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:00:05.0, compute capability: 7.5)\n",
      "2022-04-30 17:31:44.985715: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "# Define hyper parameters\n",
    "batch_size = 128\n",
    "tau = 0.1\n",
    "\n",
    "# Define base encoder and projection head\n",
    "feature_extractor = Resnet18_BaseEncoder()\n",
    "projection_head = RGBProjectionHead()\n",
    "\n",
    "# Define optimizers for contrastive learning\n",
    "pic_num_list = np.load(\"/home/ldu/CL_gaze_project/Pretrain_subject/pic_num_list.npy\")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "subject_num = 70\n",
    "\n",
    "\n",
    "# define batch for query and positive\n",
    "query_batch_y = np.zeros((batch_size, 28, 28, 6))\n",
    "query_batch_cbcr = np.zeros((batch_size, 14, 14, 6))\n",
    "positive_batch_y = np.zeros((batch_size, 28, 28, 6))\n",
    "positive_batch_cbcr = np.zeros((batch_size, 14, 14, 6))\n",
    "\n",
    "trans_list = [1, 2]\n",
    "error_list = np.zeros((70,1))\n",
    "error_list = error_list.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42f289dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7917/1914252325.py:27: UserWarning: /home/ldu/CL_gaze_project/temp_pic_0/noised_image1.jpg is a low contrast image\n",
      "  io.imsave(\"/home/ldu/CL_gaze_project/temp_pic_0/noised_image1.jpg\", noised_image1)\n",
      "/tmp/ipykernel_7917/1914252325.py:28: UserWarning: /home/ldu/CL_gaze_project/temp_pic_0/noised_image2.jpg is a low contrast image\n",
      "  io.imsave(\"/home/ldu/CL_gaze_project/temp_pic_0/noised_image2.jpg\", noised_image2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch:  0 current episode 5 current loss 0.45772427\n",
      "current epoch:  0 current episode 10 current loss 0.40635806\n",
      "current epoch:  0 current episode 15 current loss 0.31099862\n",
      "current epoch:  0 current episode 20 current loss 0.55886304\n",
      "current epoch:  0 current episode 25 current loss 0.4520669\n",
      "current epoch:  0 current episode 30 current loss 0.62375194\n",
      "current epoch:  0 current episode 35 current loss 0.5726638\n",
      "current epoch:  0 current episode 40 current loss 0.64872634\n",
      "current epoch:  0 current episode 45 current loss 0.66990083\n",
      "current epoch:  0 current episode 50 current loss 0.32806996\n",
      "current epoch:  0 current episode 55 current loss 0.6045792\n",
      "current epoch:  0 current episode 60 current loss 0.5859667\n",
      "current epoch:  0 current episode 65 current loss 0.5371083\n",
      "current epoch:  0 current episode 70 current loss 0.43211877\n",
      "current epoch:  1 current episode 5 current loss 0.54700387\n",
      "current epoch:  1 current episode 10 current loss 0.66088873\n",
      "current epoch:  1 current episode 15 current loss 0.40366548\n",
      "current epoch:  1 current episode 20 current loss 0.39967638\n",
      "current epoch:  1 current episode 25 current loss 0.35880697\n",
      "current epoch:  1 current episode 30 current loss 0.5739506\n",
      "current epoch:  1 current episode 35 current loss 0.36898205\n",
      "current epoch:  1 current episode 40 current loss 2.046259\n",
      "current epoch:  1 current episode 45 current loss 0.4579198\n",
      "current epoch:  1 current episode 50 current loss 0.5537993\n",
      "current epoch:  1 current episode 55 current loss 0.47219995\n",
      "current epoch:  1 current episode 60 current loss 0.4123231\n",
      "current epoch:  1 current episode 65 current loss 0.38709491\n",
      "current epoch:  1 current episode 70 current loss 0.565875\n",
      "current epoch:  2 current episode 5 current loss 0.89250994\n",
      "current epoch:  2 current episode 10 current loss 0.6477332\n",
      "current epoch:  2 current episode 15 current loss 0.73313904\n",
      "current epoch:  2 current episode 20 current loss 0.34068266\n",
      "current epoch:  2 current episode 25 current loss 0.49755618\n",
      "current epoch:  2 current episode 30 current loss 0.58229446\n",
      "current epoch:  2 current episode 35 current loss 0.79630613\n",
      "current epoch:  2 current episode 40 current loss 0.5876206\n",
      "current epoch:  2 current episode 45 current loss 0.4466973\n",
      "current epoch:  2 current episode 50 current loss 0.31574056\n",
      "current epoch:  2 current episode 55 current loss 0.66314644\n",
      "current epoch:  2 current episode 60 current loss 0.5164047\n",
      "current epoch:  2 current episode 65 current loss 0.41734266\n",
      "current epoch:  2 current episode 70 current loss 0.37754637\n",
      "current epoch:  3 current episode 5 current loss 0.5815465\n",
      "current epoch:  3 current episode 10 current loss 0.37257034\n",
      "current epoch:  3 current episode 15 current loss 1.4105601\n",
      "current epoch:  3 current episode 20 current loss 0.3382324\n",
      "current epoch:  3 current episode 25 current loss 0.52480364\n",
      "current epoch:  3 current episode 30 current loss 0.43752664\n",
      "current epoch:  3 current episode 35 current loss 0.61202145\n",
      "current epoch:  3 current episode 40 current loss 0.5840716\n",
      "current epoch:  3 current episode 45 current loss 0.5724622\n",
      "current epoch:  3 current episode 50 current loss 0.5173527\n",
      "current epoch:  3 current episode 55 current loss 0.35707405\n",
      "current epoch:  3 current episode 60 current loss 0.7222334\n",
      "current epoch:  3 current episode 65 current loss 0.9496465\n",
      "current epoch:  3 current episode 70 current loss 0.3907777\n",
      "current epoch:  4 current episode 5 current loss 0.82962424\n",
      "current epoch:  4 current episode 10 current loss 0.39943695\n",
      "current epoch:  4 current episode 15 current loss 1.842517\n",
      "current epoch:  4 current episode 20 current loss 0.71784526\n",
      "current epoch:  4 current episode 25 current loss 0.31198472\n",
      "current epoch:  4 current episode 30 current loss 0.43210343\n",
      "current epoch:  4 current episode 35 current loss 0.48397663\n",
      "current epoch:  4 current episode 40 current loss 0.42046937\n",
      "current epoch:  4 current episode 45 current loss 0.47174323\n",
      "current epoch:  4 current episode 50 current loss 0.97873783\n",
      "current epoch:  4 current episode 55 current loss 0.70871925\n",
      "current epoch:  4 current episode 60 current loss 0.36649087\n",
      "current epoch:  4 current episode 65 current loss 0.60366577\n",
      "current epoch:  4 current episode 70 current loss 0.3797732\n",
      "current epoch:  5 current episode 5 current loss 0.5505069\n",
      "current epoch:  5 current episode 10 current loss 0.35523793\n",
      "current epoch:  5 current episode 15 current loss 0.5367254\n",
      "current epoch:  5 current episode 20 current loss 0.5126221\n",
      "current epoch:  5 current episode 25 current loss 0.39979726\n",
      "current epoch:  5 current episode 30 current loss 0.4934832\n",
      "current epoch:  5 current episode 35 current loss 0.52321756\n",
      "current epoch:  5 current episode 40 current loss 0.37119383\n",
      "current epoch:  5 current episode 45 current loss 0.49019626\n",
      "current epoch:  5 current episode 50 current loss 0.27821746\n",
      "current epoch:  5 current episode 55 current loss 1.3341203\n",
      "current epoch:  5 current episode 60 current loss 0.3931558\n",
      "current epoch:  5 current episode 65 current loss 0.30775306\n",
      "current epoch:  5 current episode 70 current loss 0.62394005\n",
      "current epoch:  6 current episode 5 current loss 0.3987168\n",
      "current epoch:  6 current episode 10 current loss 0.29056388\n",
      "current epoch:  6 current episode 15 current loss 0.6018297\n",
      "current epoch:  6 current episode 20 current loss 0.37166435\n",
      "current epoch:  6 current episode 25 current loss 0.28125432\n",
      "current epoch:  6 current episode 30 current loss 0.7006764\n",
      "current epoch:  6 current episode 35 current loss 0.3449949\n",
      "current epoch:  6 current episode 40 current loss 0.4786782\n",
      "current epoch:  6 current episode 45 current loss 0.5138103\n",
      "current epoch:  6 current episode 50 current loss 0.7724354\n",
      "current epoch:  6 current episode 55 current loss 0.48445317\n",
      "current epoch:  6 current episode 60 current loss 0.33158344\n",
      "current epoch:  6 current episode 65 current loss 0.5729476\n",
      "current epoch:  6 current episode 70 current loss 0.99654055\n",
      "current epoch:  7 current episode 5 current loss 0.6295004\n",
      "current epoch:  7 current episode 10 current loss 0.33893847\n",
      "current epoch:  7 current episode 15 current loss 0.53483546\n",
      "current epoch:  7 current episode 20 current loss 0.4629355\n",
      "current epoch:  7 current episode 25 current loss 0.3161596\n",
      "current epoch:  7 current episode 30 current loss 0.82109153\n",
      "current epoch:  7 current episode 35 current loss 0.7725786\n",
      "current epoch:  7 current episode 40 current loss 0.45702273\n",
      "current epoch:  7 current episode 45 current loss 0.30871814\n",
      "current epoch:  7 current episode 50 current loss 0.45551705\n",
      "current epoch:  7 current episode 55 current loss 0.37889555\n",
      "current epoch:  7 current episode 60 current loss 0.618175\n",
      "current epoch:  7 current episode 65 current loss 0.377559\n",
      "current epoch:  7 current episode 70 current loss 0.46814457\n",
      "current epoch:  8 current episode 5 current loss 0.30566463\n",
      "current epoch:  8 current episode 10 current loss 0.4995836\n",
      "current epoch:  8 current episode 15 current loss 0.37403345\n",
      "current epoch:  8 current episode 20 current loss 0.3493275\n",
      "current epoch:  8 current episode 25 current loss 0.3319314\n",
      "current epoch:  8 current episode 30 current loss 0.5670779\n",
      "current epoch:  8 current episode 35 current loss 0.29545203\n",
      "current epoch:  8 current episode 40 current loss 0.55538714\n",
      "current epoch:  8 current episode 45 current loss 0.54734486\n",
      "current epoch:  8 current episode 50 current loss 0.3925309\n",
      "current epoch:  8 current episode 55 current loss 0.3950498\n",
      "current epoch:  8 current episode 60 current loss 0.98631835\n",
      "current epoch:  8 current episode 65 current loss 0.6507385\n",
      "current epoch:  8 current episode 70 current loss 0.5270889\n",
      "current epoch:  9 current episode 5 current loss 0.42443925\n",
      "current epoch:  9 current episode 10 current loss 0.28718144\n",
      "current epoch:  9 current episode 15 current loss 0.4617517\n",
      "current epoch:  9 current episode 20 current loss 0.5479478\n",
      "current epoch:  9 current episode 25 current loss 0.47406816\n",
      "current epoch:  9 current episode 30 current loss 0.66063344\n",
      "current epoch:  9 current episode 35 current loss 0.50530934\n",
      "current epoch:  9 current episode 40 current loss 0.84926176\n",
      "current epoch:  9 current episode 45 current loss 0.47024313\n",
      "current epoch:  9 current episode 50 current loss 0.42648914\n",
      "current epoch:  9 current episode 55 current loss 0.42484096\n",
      "current epoch:  9 current episode 60 current loss 0.35702503\n",
      "current epoch:  9 current episode 65 current loss 0.38512826\n",
      "current epoch:  9 current episode 70 current loss 0.5942608\n",
      "current epoch:  10 current episode 5 current loss 0.35627383\n",
      "current epoch:  10 current episode 10 current loss 0.3221768\n",
      "current epoch:  10 current episode 15 current loss 0.6769459\n",
      "current epoch:  10 current episode 20 current loss 1.963295\n",
      "current epoch:  10 current episode 25 current loss 0.52277637\n",
      "current epoch:  10 current episode 30 current loss 0.34862772\n",
      "current epoch:  10 current episode 35 current loss 0.3333894\n",
      "current epoch:  10 current episode 40 current loss 0.40638375\n",
      "current epoch:  10 current episode 45 current loss 0.57114244\n",
      "current epoch:  10 current episode 50 current loss 0.6838951\n",
      "current epoch:  10 current episode 55 current loss 0.30064213\n",
      "current epoch:  10 current episode 60 current loss 0.43706393\n",
      "current epoch:  10 current episode 65 current loss 0.4079528\n",
      "current epoch:  10 current episode 70 current loss 0.36271313\n",
      "current epoch:  11 current episode 5 current loss 0.3265369\n",
      "current epoch:  11 current episode 10 current loss 0.3963569\n",
      "current epoch:  11 current episode 15 current loss 0.62799543\n",
      "current epoch:  11 current episode 20 current loss 0.33083078\n",
      "current epoch:  11 current episode 25 current loss 1.947834\n",
      "current epoch:  11 current episode 30 current loss 0.6688807\n",
      "current epoch:  11 current episode 35 current loss 0.52159595\n",
      "current epoch:  11 current episode 40 current loss 0.6095211\n",
      "current epoch:  11 current episode 45 current loss 0.3411885\n",
      "current epoch:  11 current episode 50 current loss 0.7072668\n",
      "current epoch:  11 current episode 55 current loss 0.4185426\n",
      "current epoch:  11 current episode 60 current loss 0.42719302\n",
      "current epoch:  11 current episode 65 current loss 0.38418108\n",
      "current epoch:  11 current episode 70 current loss 0.5139512\n",
      "current epoch:  12 current episode 5 current loss 0.50286794\n",
      "current epoch:  12 current episode 10 current loss 0.6078427\n",
      "current epoch:  12 current episode 15 current loss 2.1698246\n",
      "current epoch:  12 current episode 20 current loss 0.4640841\n",
      "current epoch:  12 current episode 25 current loss 0.5287252\n",
      "current epoch:  12 current episode 30 current loss 0.3289679\n",
      "current epoch:  12 current episode 35 current loss 0.42567182\n",
      "current epoch:  12 current episode 40 current loss 0.32629395\n",
      "current epoch:  12 current episode 45 current loss 0.80461097\n",
      "current epoch:  12 current episode 50 current loss 1.0394356\n",
      "current epoch:  12 current episode 55 current loss 0.866682\n",
      "current epoch:  12 current episode 60 current loss 0.45581895\n",
      "current epoch:  12 current episode 65 current loss 1.0894644\n",
      "current epoch:  12 current episode 70 current loss 0.59697056\n",
      "current epoch:  13 current episode 5 current loss 0.35172778\n",
      "current epoch:  13 current episode 10 current loss 0.6047634\n",
      "current epoch:  13 current episode 15 current loss 0.25928587\n",
      "current epoch:  13 current episode 20 current loss 0.40749606\n",
      "current epoch:  13 current episode 25 current loss 0.3242895\n",
      "current epoch:  13 current episode 30 current loss 0.5427168\n",
      "current epoch:  13 current episode 35 current loss 0.42654398\n",
      "current epoch:  13 current episode 40 current loss 0.6246722\n",
      "current epoch:  13 current episode 45 current loss 0.68723947\n",
      "current epoch:  13 current episode 50 current loss 0.40748262\n",
      "current epoch:  13 current episode 55 current loss 0.35696808\n",
      "current epoch:  13 current episode 60 current loss 0.85576457\n",
      "current epoch:  13 current episode 65 current loss 0.34046632\n",
      "current epoch:  13 current episode 70 current loss 0.5224843\n",
      "current epoch:  14 current episode 5 current loss 0.388854\n",
      "current epoch:  14 current episode 10 current loss 0.6092227\n",
      "current epoch:  14 current episode 15 current loss 0.32758102\n",
      "current epoch:  14 current episode 20 current loss 0.6298063\n",
      "current epoch:  14 current episode 25 current loss 1.3266573\n",
      "current epoch:  14 current episode 30 current loss 0.3247056\n",
      "current epoch:  14 current episode 35 current loss 0.39816064\n",
      "current epoch:  14 current episode 40 current loss 0.48281443\n",
      "current epoch:  14 current episode 45 current loss 0.5829905\n",
      "current epoch:  14 current episode 50 current loss 0.48324132\n",
      "current epoch:  14 current episode 55 current loss 0.42011622\n",
      "current epoch:  14 current episode 60 current loss 0.5020744\n",
      "current epoch:  14 current episode 65 current loss 0.662897\n",
      "current epoch:  14 current episode 70 current loss 0.6088319\n",
      "current epoch:  15 current episode 5 current loss 0.5353662\n",
      "current epoch:  15 current episode 10 current loss 0.63111657\n",
      "current epoch:  15 current episode 15 current loss 0.4155812\n",
      "current epoch:  15 current episode 20 current loss 0.51911026\n",
      "current epoch:  15 current episode 25 current loss 0.3062467\n",
      "current epoch:  15 current episode 30 current loss 0.9610392\n",
      "current epoch:  15 current episode 35 current loss 0.4795363\n",
      "current epoch:  15 current episode 40 current loss 0.53744674\n",
      "current epoch:  15 current episode 45 current loss 0.48081422\n",
      "current epoch:  15 current episode 50 current loss 0.4936403\n",
      "current epoch:  15 current episode 55 current loss 0.44424537\n",
      "current epoch:  15 current episode 60 current loss 0.49428827\n",
      "current epoch:  15 current episode 65 current loss 0.49175668\n",
      "current epoch:  15 current episode 70 current loss 0.3402365\n",
      "current epoch:  16 current episode 5 current loss 0.41964096\n",
      "current epoch:  16 current episode 10 current loss 0.68592453\n",
      "current epoch:  16 current episode 15 current loss 0.37803653\n",
      "current epoch:  16 current episode 20 current loss 0.36654857\n",
      "current epoch:  16 current episode 25 current loss 0.39675397\n",
      "current epoch:  16 current episode 30 current loss 0.50564516\n",
      "current epoch:  16 current episode 35 current loss 0.5124788\n",
      "current epoch:  16 current episode 40 current loss 0.35243383\n",
      "current epoch:  16 current episode 45 current loss 0.6535015\n",
      "current epoch:  16 current episode 50 current loss 0.30702487\n",
      "current epoch:  16 current episode 55 current loss 0.43802488\n",
      "current epoch:  16 current episode 60 current loss 0.36845905\n",
      "current epoch:  16 current episode 65 current loss 0.5609124\n",
      "current epoch:  16 current episode 70 current loss 0.24623606\n",
      "current epoch:  17 current episode 5 current loss 0.41171774\n",
      "current epoch:  17 current episode 10 current loss 0.49071354\n",
      "current epoch:  17 current episode 15 current loss 0.3817138\n",
      "current epoch:  17 current episode 20 current loss 0.4623414\n",
      "current epoch:  17 current episode 25 current loss 0.74047565\n",
      "current epoch:  17 current episode 30 current loss 0.44973272\n",
      "current epoch:  17 current episode 35 current loss 0.4337331\n",
      "current epoch:  17 current episode 40 current loss 0.37636194\n",
      "current epoch:  17 current episode 45 current loss 0.48250747\n",
      "current epoch:  17 current episode 50 current loss 0.4006678\n",
      "current epoch:  17 current episode 55 current loss 0.46370777\n",
      "current epoch:  17 current episode 60 current loss 0.4549416\n",
      "current epoch:  17 current episode 65 current loss 0.40941262\n",
      "current epoch:  17 current episode 70 current loss 0.5729791\n",
      "current epoch:  18 current episode 5 current loss 0.648805\n",
      "current epoch:  18 current episode 10 current loss 0.352811\n",
      "current epoch:  18 current episode 15 current loss 0.37353957\n",
      "current epoch:  18 current episode 20 current loss 0.97705865\n",
      "current epoch:  18 current episode 25 current loss 0.48859063\n",
      "current epoch:  18 current episode 30 current loss 0.5810816\n",
      "current epoch:  18 current episode 35 current loss 1.0916065\n",
      "current epoch:  18 current episode 40 current loss 2.2650027\n",
      "current epoch:  18 current episode 45 current loss 0.43835938\n",
      "current epoch:  18 current episode 50 current loss 0.47614595\n",
      "current epoch:  18 current episode 55 current loss 0.36369285\n",
      "current epoch:  18 current episode 60 current loss 0.57365465\n",
      "current epoch:  18 current episode 65 current loss 0.8048687\n",
      "current epoch:  18 current episode 70 current loss 0.466947\n",
      "current epoch:  19 current episode 5 current loss 0.45822674\n",
      "current epoch:  19 current episode 10 current loss 0.74691546\n",
      "current epoch:  19 current episode 15 current loss 0.34209877\n",
      "current epoch:  19 current episode 20 current loss 1.1708914\n",
      "current epoch:  19 current episode 25 current loss 0.40828407\n",
      "current epoch:  19 current episode 30 current loss 0.48607898\n",
      "current epoch:  19 current episode 35 current loss 0.4381903\n",
      "current epoch:  19 current episode 40 current loss 0.9475757\n",
      "current epoch:  19 current episode 45 current loss 0.4559368\n",
      "current epoch:  19 current episode 50 current loss 0.32143706\n",
      "current epoch:  19 current episode 55 current loss 0.55233955\n",
      "current epoch:  19 current episode 60 current loss 0.36626628\n",
      "current epoch:  19 current episode 65 current loss 0.594022\n",
      "current epoch:  19 current episode 70 current loss 0.70191705\n",
      "current epoch:  20 current episode 5 current loss 0.46035677\n",
      "current epoch:  20 current episode 10 current loss 0.54866457\n",
      "current epoch:  20 current episode 15 current loss 0.43207067\n",
      "current epoch:  20 current episode 20 current loss 0.36906016\n",
      "current epoch:  20 current episode 25 current loss 0.53530574\n",
      "current epoch:  20 current episode 30 current loss 0.53062975\n",
      "current epoch:  20 current episode 35 current loss 0.35109052\n",
      "current epoch:  20 current episode 40 current loss 0.6600156\n",
      "current epoch:  20 current episode 45 current loss 0.39788747\n",
      "current epoch:  20 current episode 50 current loss 0.43841127\n",
      "current epoch:  20 current episode 55 current loss 0.761496\n",
      "current epoch:  20 current episode 60 current loss 0.63506424\n",
      "current epoch:  20 current episode 65 current loss 1.8569293\n",
      "current epoch:  20 current episode 70 current loss 0.7674909\n",
      "current epoch:  21 current episode 5 current loss 0.49350005\n",
      "current epoch:  21 current episode 10 current loss 0.54624164\n",
      "current epoch:  21 current episode 15 current loss 1.0545331\n",
      "current epoch:  21 current episode 20 current loss 0.6533552\n",
      "current epoch:  21 current episode 25 current loss 0.41717714\n",
      "current epoch:  21 current episode 30 current loss 0.45653182\n",
      "current epoch:  21 current episode 35 current loss 0.58733296\n",
      "current epoch:  21 current episode 40 current loss 0.74760467\n",
      "current epoch:  21 current episode 45 current loss 0.32611832\n",
      "current epoch:  21 current episode 50 current loss 0.53321403\n",
      "current epoch:  21 current episode 55 current loss 0.5478095\n",
      "current epoch:  21 current episode 60 current loss 1.0082854\n",
      "current epoch:  21 current episode 65 current loss 0.48840633\n",
      "current epoch:  21 current episode 70 current loss 0.41952604\n",
      "current epoch:  22 current episode 5 current loss 0.5092704\n",
      "current epoch:  22 current episode 10 current loss 0.65032625\n",
      "current epoch:  22 current episode 15 current loss 0.36117145\n",
      "current epoch:  22 current episode 20 current loss 0.7772573\n",
      "current epoch:  22 current episode 25 current loss 0.5145259\n",
      "current epoch:  22 current episode 30 current loss 0.54998565\n",
      "current epoch:  22 current episode 35 current loss 0.37518018\n",
      "current epoch:  22 current episode 40 current loss 0.5200697\n",
      "current epoch:  22 current episode 45 current loss 0.4577917\n",
      "current epoch:  22 current episode 50 current loss 0.51514924\n",
      "current epoch:  22 current episode 55 current loss 0.3661554\n",
      "current epoch:  22 current episode 60 current loss 0.34133464\n",
      "current epoch:  22 current episode 65 current loss 0.31916228\n",
      "current epoch:  22 current episode 70 current loss 0.3808602\n",
      "current epoch:  23 current episode 5 current loss 0.41731548\n",
      "current epoch:  23 current episode 10 current loss 0.42013866\n",
      "current epoch:  23 current episode 15 current loss 0.367559\n",
      "current epoch:  23 current episode 20 current loss 0.5621462\n",
      "current epoch:  23 current episode 25 current loss 0.56314164\n",
      "current epoch:  23 current episode 30 current loss 0.36199477\n",
      "current epoch:  23 current episode 35 current loss 0.41585737\n",
      "current epoch:  23 current episode 40 current loss 0.53749156\n",
      "current epoch:  23 current episode 45 current loss 0.52660584\n",
      "current epoch:  23 current episode 50 current loss 0.50516313\n",
      "current epoch:  23 current episode 55 current loss 0.6278198\n",
      "current epoch:  23 current episode 60 current loss 0.5303087\n",
      "current epoch:  23 current episode 65 current loss 0.5066434\n",
      "current epoch:  23 current episode 70 current loss 0.314721\n",
      "current epoch:  24 current episode 5 current loss 0.36277726\n",
      "current epoch:  24 current episode 10 current loss 0.3816619\n",
      "current epoch:  24 current episode 15 current loss 0.45542243\n",
      "current epoch:  24 current episode 20 current loss 0.47493222\n",
      "current epoch:  24 current episode 25 current loss 0.3790542\n",
      "current epoch:  24 current episode 30 current loss 0.4775818\n",
      "current epoch:  24 current episode 35 current loss 0.3307225\n",
      "current epoch:  24 current episode 40 current loss 2.0582938\n",
      "current epoch:  24 current episode 45 current loss 0.37864804\n",
      "current epoch:  24 current episode 50 current loss 0.5051634\n",
      "current epoch:  24 current episode 55 current loss 0.5091176\n",
      "current epoch:  24 current episode 60 current loss 0.46665287\n",
      "current epoch:  24 current episode 65 current loss 0.34148413\n",
      "current epoch:  24 current episode 70 current loss 0.7844768\n",
      "current epoch:  25 current episode 5 current loss 0.370804\n",
      "current epoch:  25 current episode 10 current loss 0.37200886\n",
      "current epoch:  25 current episode 15 current loss 0.3422995\n",
      "current epoch:  25 current episode 20 current loss 0.49486786\n",
      "current epoch:  25 current episode 25 current loss 0.44310048\n",
      "current epoch:  25 current episode 30 current loss 0.51580536\n",
      "current epoch:  25 current episode 35 current loss 0.44743577\n",
      "current epoch:  25 current episode 40 current loss 0.3812662\n",
      "current epoch:  25 current episode 45 current loss 0.7147453\n",
      "current epoch:  25 current episode 50 current loss 0.49211955\n",
      "current epoch:  25 current episode 55 current loss 0.38138786\n",
      "current epoch:  25 current episode 60 current loss 0.31689474\n",
      "current epoch:  25 current episode 65 current loss 0.57265365\n",
      "current epoch:  25 current episode 70 current loss 0.49017188\n",
      "current epoch:  26 current episode 5 current loss 0.5772324\n",
      "current epoch:  26 current episode 10 current loss 0.78758764\n",
      "current epoch:  26 current episode 15 current loss 0.40694395\n",
      "current epoch:  26 current episode 20 current loss 0.37615573\n",
      "current epoch:  26 current episode 25 current loss 0.3820135\n",
      "current epoch:  26 current episode 30 current loss 0.5322399\n",
      "current epoch:  26 current episode 35 current loss 0.3622889\n",
      "current epoch:  26 current episode 40 current loss 0.32326192\n",
      "current epoch:  26 current episode 45 current loss 0.49803522\n",
      "current epoch:  26 current episode 50 current loss 0.5274049\n",
      "current epoch:  26 current episode 55 current loss 0.41881734\n",
      "current epoch:  26 current episode 60 current loss 0.6081022\n",
      "current epoch:  26 current episode 65 current loss 0.36992675\n",
      "current epoch:  26 current episode 70 current loss 0.70759785\n",
      "current epoch:  27 current episode 5 current loss 0.47303307\n",
      "current epoch:  27 current episode 10 current loss 0.34841162\n",
      "current epoch:  27 current episode 15 current loss 0.5297037\n",
      "current epoch:  27 current episode 20 current loss 0.43355387\n",
      "current epoch:  27 current episode 25 current loss 0.81551725\n",
      "current epoch:  27 current episode 30 current loss 0.9788228\n",
      "current epoch:  27 current episode 35 current loss 0.37379766\n",
      "current epoch:  27 current episode 40 current loss 0.5254599\n",
      "current epoch:  27 current episode 45 current loss 0.38162956\n",
      "current epoch:  27 current episode 50 current loss 0.44942942\n",
      "current epoch:  27 current episode 55 current loss 0.39821312\n",
      "current epoch:  27 current episode 60 current loss 0.4700157\n",
      "current epoch:  27 current episode 65 current loss 0.4718653\n",
      "current epoch:  27 current episode 70 current loss 0.4659392\n",
      "current epoch:  28 current episode 5 current loss 0.38575098\n",
      "current epoch:  28 current episode 10 current loss 0.44284832\n",
      "current epoch:  28 current episode 15 current loss 0.5360519\n",
      "current epoch:  28 current episode 20 current loss 0.42103046\n",
      "current epoch:  28 current episode 25 current loss 0.43871844\n",
      "current epoch:  28 current episode 30 current loss 0.6194918\n",
      "current epoch:  28 current episode 35 current loss 0.45773703\n",
      "current epoch:  28 current episode 40 current loss 0.4370555\n",
      "current epoch:  28 current episode 45 current loss 0.5430831\n",
      "current epoch:  28 current episode 50 current loss 0.63967264\n",
      "current epoch:  28 current episode 55 current loss 0.4071446\n",
      "current epoch:  28 current episode 60 current loss 0.830979\n",
      "current epoch:  28 current episode 65 current loss 0.41670626\n",
      "current epoch:  28 current episode 70 current loss 0.39529973\n",
      "current epoch:  29 current episode 5 current loss 0.40440795\n",
      "current epoch:  29 current episode 10 current loss 0.48176682\n",
      "current epoch:  29 current episode 15 current loss 0.3674966\n",
      "current epoch:  29 current episode 20 current loss 0.3888998\n",
      "current epoch:  29 current episode 25 current loss 1.3050914\n",
      "current epoch:  29 current episode 30 current loss 0.40877753\n",
      "current epoch:  29 current episode 35 current loss 0.91406816\n",
      "current epoch:  29 current episode 40 current loss 0.53099895\n",
      "current epoch:  29 current episode 45 current loss 0.44759458\n",
      "current epoch:  29 current episode 50 current loss 0.36332998\n",
      "current epoch:  29 current episode 55 current loss 0.748991\n",
      "current epoch:  29 current episode 60 current loss 0.4989411\n",
      "current epoch:  29 current episode 65 current loss 0.44270235\n",
      "current epoch:  29 current episode 70 current loss 0.6139214\n",
      "current epoch:  30 current episode 5 current loss 0.43625224\n",
      "current epoch:  30 current episode 10 current loss 0.30240676\n",
      "current epoch:  30 current episode 15 current loss 0.33645812\n",
      "current epoch:  30 current episode 20 current loss 0.3256497\n",
      "current epoch:  30 current episode 25 current loss 0.40321517\n",
      "current epoch:  30 current episode 30 current loss 0.46693277\n",
      "current epoch:  30 current episode 35 current loss 0.3696606\n",
      "current epoch:  30 current episode 40 current loss 2.1996822\n",
      "current epoch:  30 current episode 45 current loss 0.49876156\n",
      "current epoch:  30 current episode 50 current loss 0.37769893\n",
      "current epoch:  30 current episode 55 current loss 0.47685322\n",
      "current epoch:  30 current episode 60 current loss 0.6882386\n",
      "current epoch:  30 current episode 65 current loss 0.5238831\n",
      "current epoch:  30 current episode 70 current loss 0.5731202\n",
      "current epoch:  31 current episode 5 current loss 0.49489045\n",
      "current epoch:  31 current episode 10 current loss 0.41676086\n",
      "current epoch:  31 current episode 15 current loss 0.41010624\n",
      "current epoch:  31 current episode 20 current loss 0.3658266\n",
      "current epoch:  31 current episode 25 current loss 0.46809685\n",
      "current epoch:  31 current episode 30 current loss 0.55550516\n",
      "current epoch:  31 current episode 35 current loss 0.5614443\n",
      "current epoch:  31 current episode 40 current loss 1.3497225\n",
      "current epoch:  31 current episode 45 current loss 0.44312745\n",
      "current epoch:  31 current episode 50 current loss 0.61197436\n",
      "current epoch:  31 current episode 55 current loss 0.45447612\n",
      "current epoch:  31 current episode 60 current loss 1.0389497\n",
      "current epoch:  31 current episode 65 current loss 0.5970853\n",
      "current epoch:  31 current episode 70 current loss 0.38518384\n",
      "current epoch:  32 current episode 5 current loss 0.32438588\n",
      "current epoch:  32 current episode 10 current loss 0.56936216\n",
      "current epoch:  32 current episode 15 current loss 0.38748455\n",
      "current epoch:  32 current episode 20 current loss 0.5484015\n",
      "current epoch:  32 current episode 25 current loss 0.5618228\n",
      "current epoch:  32 current episode 30 current loss 0.3871317\n",
      "current epoch:  32 current episode 35 current loss 0.5181962\n",
      "current epoch:  32 current episode 40 current loss 0.48071656\n",
      "current epoch:  32 current episode 45 current loss 0.40054506\n",
      "current epoch:  32 current episode 50 current loss 0.32272142\n",
      "current epoch:  32 current episode 55 current loss 0.669296\n",
      "current epoch:  32 current episode 60 current loss 0.43185186\n",
      "current epoch:  32 current episode 65 current loss 0.47235018\n",
      "current epoch:  32 current episode 70 current loss 0.5477817\n",
      "current epoch:  33 current episode 5 current loss 0.39080897\n",
      "current epoch:  33 current episode 10 current loss 0.73565984\n",
      "current epoch:  33 current episode 15 current loss 1.4456072\n",
      "current epoch:  33 current episode 20 current loss 0.36783105\n",
      "current epoch:  33 current episode 25 current loss 0.3956729\n",
      "current epoch:  33 current episode 30 current loss 0.33875018\n",
      "current epoch:  33 current episode 35 current loss 0.5741735\n",
      "current epoch:  33 current episode 40 current loss 0.3533181\n",
      "current epoch:  33 current episode 45 current loss 0.399978\n",
      "current epoch:  33 current episode 50 current loss 0.3993763\n",
      "current epoch:  33 current episode 55 current loss 0.99818325\n",
      "current epoch:  33 current episode 60 current loss 0.414717\n",
      "current epoch:  33 current episode 65 current loss 0.84334683\n",
      "current epoch:  33 current episode 70 current loss 0.5981835\n",
      "current epoch:  34 current episode 5 current loss 0.44103128\n",
      "current epoch:  34 current episode 10 current loss 0.51366353\n",
      "current epoch:  34 current episode 15 current loss 1.6965652\n",
      "current epoch:  34 current episode 20 current loss 0.34365773\n",
      "current epoch:  34 current episode 25 current loss 0.34731102\n",
      "current epoch:  34 current episode 30 current loss 0.3849103\n",
      "current epoch:  34 current episode 35 current loss 0.5025257\n",
      "current epoch:  34 current episode 40 current loss 0.48231578\n",
      "current epoch:  34 current episode 45 current loss 0.3942145\n",
      "current epoch:  34 current episode 50 current loss 0.41391882\n",
      "current epoch:  34 current episode 55 current loss 0.6240459\n",
      "current epoch:  34 current episode 60 current loss 0.32263494\n",
      "current epoch:  34 current episode 65 current loss 0.49361503\n",
      "current epoch:  34 current episode 70 current loss 0.59590685\n",
      "current epoch:  35 current episode 5 current loss 0.4225034\n",
      "current epoch:  35 current episode 10 current loss 0.5028125\n",
      "current epoch:  35 current episode 15 current loss 0.41716468\n",
      "current epoch:  35 current episode 20 current loss 0.33708873\n",
      "current epoch:  35 current episode 25 current loss 0.3296941\n",
      "current epoch:  35 current episode 30 current loss 0.42786998\n",
      "current epoch:  35 current episode 35 current loss 0.506737\n",
      "current epoch:  35 current episode 40 current loss 1.0884334\n",
      "current epoch:  35 current episode 45 current loss 0.6929668\n",
      "current epoch:  35 current episode 50 current loss 0.6110414\n",
      "current epoch:  35 current episode 55 current loss 0.7405815\n",
      "current epoch:  35 current episode 60 current loss 0.53858066\n",
      "current epoch:  35 current episode 65 current loss 0.54122657\n",
      "current epoch:  35 current episode 70 current loss 0.69380736\n",
      "current epoch:  36 current episode 5 current loss 0.4959157\n",
      "current epoch:  36 current episode 10 current loss 0.6091199\n",
      "current epoch:  36 current episode 15 current loss 0.40302607\n",
      "current epoch:  36 current episode 20 current loss 0.61332154\n",
      "current epoch:  36 current episode 25 current loss 0.5076786\n",
      "current epoch:  36 current episode 30 current loss 0.3890655\n",
      "current epoch:  36 current episode 35 current loss 0.9012146\n",
      "current epoch:  36 current episode 40 current loss 0.4894689\n",
      "current epoch:  36 current episode 45 current loss 0.46887037\n",
      "current epoch:  36 current episode 50 current loss 0.5601523\n",
      "current epoch:  36 current episode 55 current loss 1.5042502\n",
      "current epoch:  36 current episode 60 current loss 0.53705287\n",
      "current epoch:  36 current episode 65 current loss 0.4643559\n",
      "current epoch:  36 current episode 70 current loss 0.38230264\n",
      "current epoch:  37 current episode 5 current loss 0.35189307\n",
      "current epoch:  37 current episode 10 current loss 0.30771255\n",
      "current epoch:  37 current episode 15 current loss 1.3105149\n",
      "current epoch:  37 current episode 20 current loss 0.5342288\n",
      "current epoch:  37 current episode 25 current loss 0.6410149\n",
      "current epoch:  37 current episode 30 current loss 0.2903358\n",
      "current epoch:  37 current episode 35 current loss 0.45869142\n",
      "current epoch:  37 current episode 40 current loss 0.46704578\n",
      "current epoch:  37 current episode 45 current loss 0.52065295\n",
      "current epoch:  37 current episode 50 current loss 0.62712204\n",
      "current epoch:  37 current episode 55 current loss 0.46506673\n",
      "current epoch:  37 current episode 60 current loss 0.5077766\n",
      "current epoch:  37 current episode 65 current loss 0.46979892\n",
      "current epoch:  37 current episode 70 current loss 0.48100293\n",
      "current epoch:  38 current episode 5 current loss 0.5773893\n",
      "current epoch:  38 current episode 10 current loss 0.63745415\n",
      "current epoch:  38 current episode 15 current loss 0.45188951\n",
      "current epoch:  38 current episode 20 current loss 0.50025207\n",
      "current epoch:  38 current episode 25 current loss 0.3167597\n",
      "current epoch:  38 current episode 30 current loss 0.36528504\n",
      "current epoch:  38 current episode 35 current loss 0.6298322\n",
      "current epoch:  38 current episode 40 current loss 0.45913917\n",
      "current epoch:  38 current episode 45 current loss 0.359198\n",
      "current epoch:  38 current episode 50 current loss 0.6681632\n",
      "current epoch:  38 current episode 55 current loss 0.3764356\n",
      "current epoch:  38 current episode 60 current loss 0.29760835\n",
      "current epoch:  38 current episode 65 current loss 0.37990674\n",
      "current epoch:  38 current episode 70 current loss 0.47733173\n",
      "current epoch:  39 current episode 5 current loss 0.40863714\n",
      "current epoch:  39 current episode 10 current loss 0.5633509\n",
      "current epoch:  39 current episode 15 current loss 0.6379789\n",
      "current epoch:  39 current episode 20 current loss 0.48818237\n",
      "current epoch:  39 current episode 25 current loss 0.92380655\n",
      "current epoch:  39 current episode 30 current loss 0.500802\n",
      "current epoch:  39 current episode 35 current loss 0.4140693\n",
      "current epoch:  39 current episode 40 current loss 0.34410945\n",
      "current epoch:  39 current episode 45 current loss 0.39915365\n",
      "current epoch:  39 current episode 50 current loss 0.2643777\n",
      "current epoch:  39 current episode 55 current loss 0.61597073\n",
      "current epoch:  39 current episode 60 current loss 0.4907115\n",
      "current epoch:  39 current episode 65 current loss 0.41780907\n",
      "current epoch:  39 current episode 70 current loss 0.33575535\n",
      "current epoch:  40 current episode 5 current loss 0.49892247\n",
      "current epoch:  40 current episode 10 current loss 0.6587293\n",
      "current epoch:  40 current episode 15 current loss 0.5415144\n",
      "current epoch:  40 current episode 20 current loss 0.47696772\n",
      "current epoch:  40 current episode 25 current loss 0.42468533\n",
      "current epoch:  40 current episode 30 current loss 0.34035826\n",
      "current epoch:  40 current episode 35 current loss 0.67756164\n",
      "current epoch:  40 current episode 40 current loss 0.53095484\n",
      "current epoch:  40 current episode 45 current loss 0.42954916\n",
      "current epoch:  40 current episode 50 current loss 0.29243922\n",
      "current epoch:  40 current episode 55 current loss 1.9085516\n",
      "current epoch:  40 current episode 60 current loss 0.4917636\n",
      "current epoch:  40 current episode 65 current loss 0.5482672\n",
      "current epoch:  40 current episode 70 current loss 0.39556903\n",
      "current epoch:  41 current episode 5 current loss 0.45729348\n",
      "current epoch:  41 current episode 10 current loss 0.48845735\n",
      "current epoch:  41 current episode 15 current loss 0.5732544\n",
      "current epoch:  41 current episode 20 current loss 0.31048363\n",
      "current epoch:  41 current episode 25 current loss 0.252869\n",
      "current epoch:  41 current episode 30 current loss 0.36096096\n",
      "current epoch:  41 current episode 35 current loss 0.34265548\n",
      "current epoch:  41 current episode 40 current loss 0.38634092\n",
      "current epoch:  41 current episode 45 current loss 0.4609006\n",
      "current epoch:  41 current episode 50 current loss 0.89162254\n",
      "current epoch:  41 current episode 55 current loss 0.5951176\n",
      "current epoch:  41 current episode 60 current loss 0.34092933\n",
      "current epoch:  41 current episode 65 current loss 0.55095714\n",
      "current epoch:  41 current episode 70 current loss 0.5434756\n",
      "current epoch:  42 current episode 5 current loss 1.5996937\n",
      "current epoch:  42 current episode 10 current loss 0.39480782\n",
      "current epoch:  42 current episode 15 current loss 0.70848227\n",
      "current epoch:  42 current episode 20 current loss 0.52211356\n",
      "current epoch:  42 current episode 25 current loss 0.42784983\n",
      "current epoch:  42 current episode 30 current loss 0.6503365\n",
      "current epoch:  42 current episode 35 current loss 0.36887014\n",
      "current epoch:  42 current episode 40 current loss 0.40254948\n",
      "current epoch:  42 current episode 45 current loss 0.40228108\n",
      "current epoch:  42 current episode 50 current loss 0.35845146\n",
      "current epoch:  42 current episode 55 current loss 0.3321547\n",
      "current epoch:  42 current episode 60 current loss 0.5181676\n",
      "current epoch:  42 current episode 65 current loss 0.55186754\n",
      "current epoch:  42 current episode 70 current loss 0.43408555\n",
      "current epoch:  43 current episode 5 current loss 0.47750068\n",
      "current epoch:  43 current episode 10 current loss 0.40264\n",
      "current epoch:  43 current episode 15 current loss 0.50826776\n",
      "current epoch:  43 current episode 20 current loss 0.64183164\n",
      "current epoch:  43 current episode 25 current loss 0.46745265\n",
      "current epoch:  43 current episode 30 current loss 0.49383825\n",
      "current epoch:  43 current episode 35 current loss 0.8947543\n",
      "current epoch:  43 current episode 40 current loss 0.45563033\n",
      "current epoch:  43 current episode 45 current loss 0.4224103\n",
      "current epoch:  43 current episode 50 current loss 0.4174422\n",
      "current epoch:  43 current episode 55 current loss 0.33062303\n",
      "current epoch:  43 current episode 60 current loss 0.3988791\n",
      "current epoch:  43 current episode 65 current loss 0.44600558\n",
      "current epoch:  43 current episode 70 current loss 0.40441337\n",
      "current epoch:  44 current episode 5 current loss 0.6727073\n",
      "current epoch:  44 current episode 10 current loss 1.0340011\n",
      "current epoch:  44 current episode 15 current loss 0.43056118\n",
      "current epoch:  44 current episode 20 current loss 0.78211\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7917/1914252325.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mnoised_image1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoised_image1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mnoised_image2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoised_image2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mnoised_image1\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "# contrastive learning\n",
    "for epoch in range(100):\n",
    "\n",
    "    subject_index = np.random.choice(subject_num, subject_num, replace=False)\n",
    "    episode = 0\n",
    "    for sub in subject_index:\n",
    "        index = np.random.choice(pic_num_list[sub], batch_size, replace=False)\n",
    "        j = 0\n",
    "        for i in index:\n",
    "            jpeg_file = '/home/ldu/CL_gaze_project/Pretrain_subject/' + str(sub) + '/' + str(i) + '.jpg'\n",
    "            img = io.imread(jpeg_file) / 255.\n",
    "            selected_trans = random.sample(trans_list, 2)\n",
    "            noised_image1 = trans_in_rgb.apply_transformation(copy.deepcopy(img), selected_trans[0])\n",
    "            noised_image2 = trans_in_rgb.apply_transformation(copy.deepcopy(img), selected_trans[1])\n",
    "\n",
    "            noised_image1 = trans_in_rgb.apply_transformation(copy.deepcopy(noised_image1), selected_trans[1])\n",
    "            noised_image2 = trans_in_rgb.apply_transformation(copy.deepcopy(noised_image2), selected_trans[0])\n",
    "            \n",
    "            noised_image1 = cv2.resize(noised_image1, (224,224))\n",
    "            noised_image2 = cv2.resize(noised_image2, (224,224))\n",
    "\n",
    "            noised_image1 *= 255\n",
    "            noised_image1 = np.round(noised_image1).astype(np.uint8)\n",
    "            noised_image2 *= 255\n",
    "            noised_image2 = np.round(noised_image2).astype(np.uint8)\n",
    "            io.imsave(\"/home/ldu/CL_gaze_project/temp_pic_0/noised_image1.jpg\", noised_image1)\n",
    "            io.imsave(\"/home/ldu/CL_gaze_project/temp_pic_0/noised_image2.jpg\", noised_image2)\n",
    "\n",
    "            jpeg_noised = \"/home/ldu/CL_gaze_project/temp_pic_0/noised_image1.jpg\"\n",
    "            dct_y, dct_cb, dct_cr = load(jpeg_noised)\n",
    "            # channel selection\n",
    "            dct_y = np.concatenate((np.concatenate((dct_y[:, :, 0:3], dct_y[:, :, 8:10]), axis=2),\n",
    "                                    np.reshape(dct_y[:, :, 16], (28, 28, 1))), axis=2)\n",
    "\n",
    "            dct_cb = np.concatenate((dct_cb[:, :, 0:2], np.reshape(dct_cb[:, :, 8], (14, 14, 1))), axis=2)\n",
    "            dct_cr = np.concatenate((dct_cr[:, :, 0:2], np.reshape(dct_cr[:, :, 8], (14, 14, 1))), axis=2)\n",
    "            cb_cr = np.concatenate([dct_cb, dct_cr], 2)\n",
    "            query_batch_y[j, :, :, :] = dct_y\n",
    "            query_batch_cbcr[j, :, :, :] = cb_cr\n",
    "\n",
    "            jpeg_noised = \"/home/ldu/CL_gaze_project/temp_pic_0/noised_image2.jpg\"\n",
    "            dct_y, dct_cb, dct_cr = load(jpeg_noised)\n",
    "            # channel selection\n",
    "            dct_y = np.concatenate((np.concatenate((dct_y[:, :, 0:3], dct_y[:, :, 8:10]), axis=2),\n",
    "                                    np.reshape(dct_y[:, :, 16], (28, 28, 1))), axis=2)\n",
    "\n",
    "            dct_cb = np.concatenate((dct_cb[:, :, 0:2], np.reshape(dct_cb[:, :, 8], (14, 14, 1))), axis=2)\n",
    "            dct_cr = np.concatenate((dct_cr[:, :, 0:2], np.reshape(dct_cr[:, :, 8], (14, 14, 1))), axis=2)\n",
    "            cb_cr = np.concatenate([dct_cb, dct_cr], 2)\n",
    "            positive_batch_y[j, :, :, :] = dct_y\n",
    "            positive_batch_cbcr[j, :, :, :] = cb_cr\n",
    "\n",
    "            j += 1\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            x1_feature = tf.math.l2_normalize(projection_head(feature_extractor([query_batch_y, query_batch_cbcr], training=True), training=True), axis=1)\n",
    "            x2_feature = tf.math.l2_normalize(projection_head(feature_extractor([positive_batch_y, positive_batch_cbcr], training=True), training=True), axis=1)\n",
    "\n",
    "            x1_x2_mat = tf.exp(tf.matmul(x1_feature, tf.transpose(x2_feature)) / tau)\n",
    "\n",
    "            denominator = tf.reduce_sum(x1_x2_mat, 1)\n",
    "            #positive_sim = tf.linalg.diag_part(x1_x2_mat)\n",
    "            prob = x1_x2_mat / tf.reshape(denominator, (x1_feature.shape[0], 1))\n",
    "            prob = tf.linalg.diag_part(prob)\n",
    "\n",
    "            loss = -tf.reduce_mean(tf.math.log(prob))\n",
    "\n",
    "        grads = tape.gradient(loss, feature_extractor.trainable_variables + projection_head.trainable_variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, feature_extractor.trainable_variables + projection_head.trainable_variables))\n",
    "        episode += 1\n",
    "        error_list[sub].append(loss.numpy())\n",
    "        if episode % 5 == 0:\n",
    "            print(\"current epoch: \", epoch, \"current episode\", episode, \"current loss\", loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6b70b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7917/1178378646.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  error_list_arr= np.array(error_list)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_list_arr= np.array(error_list)\n",
    "for sub_fig in range(20):\n",
    "    plt.plot(error_list_arr[sub_fig][1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c93b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "061c6383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_extractor.save_weights(\"/home/ldu/CL_gaze_project/dct_subject_late_concat_simple_input224_res18/base_1\")\n",
    "#projection_head.save_weights(\"/home/ldu/CL_gaze_project/dct_subject_late_concat_simple_input224_res18/pro_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ac1a2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe4201506d0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor.load_weights(\"/home/ldu/CL_gaze_project/dct_subject_late_concat_simple_input224_res18/base_1\")\n",
    "projection_head.load_weights(\"/home/ldu/CL_gaze_project/dct_subject_late_concat_simple_input224_res18/pro_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7bfc2806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573\n"
     ]
    }
   ],
   "source": [
    "labels_per = np.load(\"/home/ldu/CL_gaze_project/Personalize/subject0102/labels.npy\")\n",
    "GazeEstimation = CL_model.GazeEstimationHead()\n",
    "GazeEstimation.load_weights(\"/home/ldu/CL_gaze_project/dct_mix_late_concat_new_structure/Gaze_1\")\n",
    "data_size = labels_per.shape[0]\n",
    "print(data_size)\n",
    "\n",
    "optimizer_per = tf.keras.optimizers.Adam(learning_rate=.0025, decay=0.0005)\n",
    "\n",
    "train_error_per = []\n",
    "val_error_per = []\n",
    "test_error_per = []\n",
    "\n",
    "total_index = np.load(\"/home/ldu/CL_gaze_project/Personalize/subject0102/random_index.npy\")\n",
    "train_index = total_index[75: 150]\n",
    "val_index = total_index[0: 25]\n",
    "test_index = np.concatenate((total_index[25:75],total_index[150: data_size]),0)\n",
    "#print(train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "edf9c78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 loss 0.2316558 train 35.130281433666156 validation 127.6271545207371\n",
      "epoch: 0 test error: 130.4741518576514\n",
      "epoch:  5 loss 2.1227992 train 98.90245894943652 validation 77.85734074073439\n",
      "epoch: 5 test error: 73.82085757939704\n",
      "epoch:  10 loss 0.0945562 train 22.837636818957073 validation 24.05441477017171\n",
      "epoch: 10 test error: 24.939713033927053\n",
      "epoch:  15 loss 0.123295024 train 24.89534027886137 validation 24.648369804737982\n",
      "epoch: 15 test error: 26.385454130259717\n",
      "epoch:  20 loss 0.11329956 train 23.34055418570159 validation 22.753997523070044\n",
      "epoch: 20 test error: 24.305006592387326\n",
      "epoch:  25 loss 0.081804514 train 20.385454838181317 validation 20.862961395259482\n",
      "epoch: 25 test error: 21.38355490785494\n",
      "epoch:  30 loss 0.058674343 train 17.42190329960698 validation 19.72154508145429\n",
      "epoch: 30 test error: 18.015524655078003\n",
      "epoch:  35 loss 0.04895277 train 15.32629001849787 validation 17.58209407832409\n",
      "epoch: 35 test error: 15.93556437114065\n",
      "epoch:  40 loss 0.037946805 train 13.281812833932797 validation 14.97005928054575\n",
      "epoch: 40 test error: 13.673356901769619\n",
      "epoch:  45 loss 0.024671126 train 10.730331765940957 validation 11.96932973500991\n",
      "epoch: 45 test error: 10.760965214976716\n",
      "epoch:  50 loss 0.014172619 train 7.879017053537007 validation 9.502311588679365\n",
      "epoch: 50 test error: 8.92759815659814\n",
      "epoch:  55 loss 0.0091213025 train 6.390137941874328 validation 7.644981602251773\n",
      "epoch: 55 test error: 7.668469462149405\n",
      "epoch:  60 loss 0.007305694 train 5.649278514145417 validation 6.866912045892985\n",
      "epoch: 60 test error: 7.1136754611201285\n",
      "epoch:  65 loss 0.006602211 train 5.397527486478204 validation 6.4018533066588414\n",
      "epoch: 65 test error: 6.54891138222294\n",
      "epoch:  70 loss 0.0057583805 train 5.033342674176618 validation 6.046145769505028\n",
      "epoch: 70 test error: 5.992430407685703\n",
      "epoch:  75 loss 0.004669818 train 4.543600081700066 validation 5.581709829703809\n",
      "epoch: 75 test error: 5.5461059620367505\n",
      "epoch:  80 loss 0.0038669833 train 4.101974781704931 validation 4.969627156564984\n",
      "epoch: 80 test error: 5.29214200783667\n",
      "epoch:  85 loss 0.0034643293 train 3.808635299165952 validation 4.679893164639833\n",
      "epoch: 85 test error: 5.177987957329779\n",
      "epoch:  90 loss 0.0031506452 train 3.5155679040392025 validation 4.543142879226016\n",
      "epoch: 90 test error: 5.023212609975163\n",
      "epoch:  95 loss 0.0028206783 train 3.291119074884912 validation 4.452888251170769\n",
      "epoch: 95 test error: 4.796533794752432\n",
      "epoch:  100 loss 0.0025620277 train 3.120806187575694 validation 4.3688524229040375\n",
      "epoch: 100 test error: 4.604987898708609\n",
      "epoch:  105 loss 0.0023320324 train 2.9617452611872226 validation 4.232995103647751\n",
      "epoch: 105 test error: 4.436666569218232\n",
      "epoch:  110 loss 0.0021449642 train 2.8262740890746465 validation 4.120376339806208\n",
      "epoch: 110 test error: 4.313891755869499\n",
      "epoch:  115 loss 0.001987306 train 2.71086829525498 validation 4.012659824852578\n",
      "epoch: 115 test error: 4.216864330388216\n",
      "epoch:  120 loss 0.0018430052 train 2.6066754623352795 validation 3.94977725532736\n",
      "epoch: 120 test error: 4.124673815151633\n",
      "epoch:  125 loss 0.0017207436 train 2.512046567929642 validation 3.873439492016086\n",
      "epoch: 125 test error: 4.038156398539378\n",
      "epoch:  130 loss 0.0016120127 train 2.4253625259842204 validation 3.8069492294884686\n",
      "epoch: 130 test error: 3.9673350272393377\n",
      "epoch:  135 loss 0.0015125463 train 2.340851071841258 validation 3.7334637592051854\n",
      "epoch: 135 test error: 3.8869027563627956\n",
      "epoch:  140 loss 0.0014280361 train 2.268249954428883 validation 3.691232982052002\n",
      "epoch: 140 test error: 3.8279613042714966\n",
      "epoch:  145 loss 0.0013480549 train 2.203598662762148 validation 3.671867999499858\n",
      "epoch: 145 test error: 3.78163789013118\n",
      "epoch:  150 loss 0.0012820346 train 2.1505238604873935 validation 3.637786984206316\n",
      "epoch: 150 test error: 3.7207788973431595\n",
      "epoch:  155 loss 0.001218224 train 2.100407622041949 validation 3.615360246681297\n",
      "epoch: 155 test error: 3.675343257863424\n",
      "epoch:  160 loss 0.0011576379 train 2.0507344630668998 validation 3.585066661358031\n",
      "epoch: 160 test error: 3.6327569846183065\n",
      "epoch:  165 loss 0.0011049376 train 2.0060462257162226 validation 3.5769949696284984\n",
      "epoch: 165 test error: 3.601931604710736\n",
      "epoch:  170 loss 0.001055492 train 1.9656718191654072 validation 3.5537100074329784\n",
      "epoch: 170 test error: 3.56330010009186\n",
      "epoch:  175 loss 0.001011633 train 1.9353729110360929 validation 3.555867734950218\n",
      "epoch: 175 test error: 3.541117863007761\n",
      "epoch:  180 loss 0.0009755245 train 1.910448435750171 validation 3.5457866409539487\n",
      "epoch: 180 test error: 3.5195169280356176\n",
      "epoch:  185 loss 0.0009393723 train 1.886010146482113 validation 3.5533511881825928\n",
      "epoch: 185 test error: 3.508545276876398\n",
      "epoch:  190 loss 0.0009039818 train 1.8587213886765501 validation 3.5567194949417944\n",
      "epoch: 190 test error: 3.4986062074559383\n",
      "epoch:  195 loss 0.00086868653 train 1.8255278023449368 validation 3.564237047151347\n",
      "epoch: 195 test error: 3.490760551901695\n",
      "epoch:  200 loss 0.0008355261 train 1.7942584231013081 validation 3.5726981195050405\n",
      "epoch: 200 test error: 3.4829877953387856\n",
      "epoch:  205 loss 0.0008082405 train 1.7696878653242027 validation 3.5692737930472247\n",
      "epoch: 205 test error: 3.465901310109361\n",
      "epoch:  210 loss 0.00078503916 train 1.750946533997491 validation 3.573668985369644\n",
      "epoch: 210 test error: 3.4491303324611917\n",
      "epoch:  215 loss 0.000759933 train 1.7261086283258096 validation 3.5580581497998462\n",
      "epoch: 215 test error: 3.4282698573566357\n",
      "epoch:  220 loss 0.00073692907 train 1.7064615030461527 validation 3.577804389027207\n",
      "epoch: 220 test error: 3.421037447382496\n",
      "epoch:  225 loss 0.0007120423 train 1.6796463956329146 validation 3.5668231134761625\n",
      "epoch: 225 test error: 3.408893898155321\n",
      "epoch:  230 loss 0.00068929925 train 1.6552663061666626 validation 3.5843248681670037\n",
      "epoch: 230 test error: 3.407634414093668\n",
      "epoch:  235 loss 0.0006703131 train 1.6370186114241332 validation 3.598536123889051\n",
      "epoch: 235 test error: 3.409424802470974\n",
      "epoch:  240 loss 0.000650611 train 1.611480838394989 validation 3.5913563128685104\n",
      "epoch: 240 test error: 3.4003574133067724\n",
      "epoch:  245 loss 0.00063309184 train 1.5926261202920116 validation 3.6051225008015626\n",
      "epoch: 245 test error: 3.4011963293185903\n",
      "epoch:  250 loss 0.00061576423 train 1.57034450527965 validation 3.605122282242836\n",
      "epoch: 250 test error: 3.3926983480959763\n",
      "epoch:  255 loss 0.0006001173 train 1.550818689192915 validation 3.598118943557529\n",
      "epoch: 255 test error: 3.383108614063502\n",
      "epoch:  260 loss 0.0005850536 train 1.5328382758729022 validation 3.6168603140735542\n",
      "epoch: 260 test error: 3.3877737587498737\n",
      "epoch:  265 loss 0.00057085743 train 1.5133118158773429 validation 3.610780674956396\n",
      "epoch: 265 test error: 3.3803535129248816\n",
      "epoch:  270 loss 0.000556822 train 1.495447063972891 validation 3.6103389951394673\n",
      "epoch: 270 test error: 3.3783580242525635\n",
      "epoch:  275 loss 0.00054394203 train 1.477654478678198 validation 3.6153367231272147\n",
      "epoch: 275 test error: 3.378300285722696\n",
      "epoch:  280 loss 0.00053164916 train 1.4596451892218776 validation 3.6078550923620134\n",
      "epoch: 280 test error: 3.372680556166899\n",
      "epoch:  285 loss 0.00052012363 train 1.4434801936787214 validation 3.6159023681022857\n",
      "epoch: 285 test error: 3.37503260570298\n",
      "epoch:  290 loss 0.0005086992 train 1.4257899612586444 validation 3.6115839399012235\n",
      "epoch: 290 test error: 3.3715885408332973\n",
      "epoch:  295 loss 0.00049791875 train 1.4103259020043561 validation 3.6098935083826977\n",
      "epoch: 295 test error: 3.3717842477850732\n",
      "epoch:  300 loss 0.0004878778 train 1.3954567847020238 validation 3.610892002860835\n",
      "epoch: 300 test error: 3.3697071188050987\n",
      "epoch:  305 loss 0.00047794194 train 1.379453419624715 validation 3.6054813318444316\n",
      "epoch: 305 test error: 3.3681433849702573\n",
      "epoch:  310 loss 0.0004685249 train 1.3651470807669368 validation 3.6036834607196795\n",
      "epoch: 310 test error: 3.3670223691592707\n",
      "epoch:  315 loss 0.00045953417 train 1.3502258341511884 validation 3.601931437546172\n",
      "epoch: 315 test error: 3.366212635577489\n",
      "epoch:  320 loss 0.00045084453 train 1.3372759235468308 validation 3.604359878736401\n",
      "epoch: 320 test error: 3.3680933620896067\n",
      "epoch:  325 loss 0.00044225247 train 1.3227004821866002 validation 3.60271218808806\n",
      "epoch: 325 test error: 3.367738240232805\n",
      "epoch:  330 loss 0.00043416297 train 1.311326761989806 validation 3.5965657917920533\n",
      "epoch: 330 test error: 3.3653383041817917\n",
      "epoch:  335 loss 0.00042623034 train 1.296632832181481 validation 3.593018925570152\n",
      "epoch: 335 test error: 3.3635216779672277\n",
      "epoch:  340 loss 0.0004185592 train 1.2833711647702373 validation 3.58918651780951\n",
      "epoch: 340 test error: 3.362745939309941\n",
      "epoch:  345 loss 0.0004112797 train 1.2715119386389164 validation 3.5955891024404707\n",
      "epoch: 345 test error: 3.364822725363683\n",
      "epoch:  350 loss 0.0004041072 train 1.2592105910698985 validation 3.5855531864960652\n",
      "epoch: 350 test error: 3.3625168435263157\n",
      "epoch:  355 loss 0.00039728283 train 1.2485384408628235 validation 3.5888598866906887\n",
      "epoch: 355 test error: 3.3642393365632537\n",
      "epoch:  360 loss 0.00039046665 train 1.2357191772299316 validation 3.5769659939828706\n",
      "epoch: 360 test error: 3.361320616241511\n",
      "epoch:  365 loss 0.00038394178 train 1.2250794772184086 validation 3.575376785028882\n",
      "epoch: 365 test error: 3.3614990783629795\n",
      "epoch:  370 loss 0.0003775606 train 1.2141887867666534 validation 3.5779689931129384\n",
      "epoch: 370 test error: 3.3622929049085974\n",
      "epoch:  375 loss 0.00037126476 train 1.2030822216354473 validation 3.5730462704945727\n",
      "epoch: 375 test error: 3.362575471347638\n",
      "epoch:  380 loss 0.00036522545 train 1.1931633580006387 validation 3.5672380215617268\n",
      "epoch: 380 test error: 3.3612686519403723\n",
      "epoch:  385 loss 0.00035921638 train 1.1828898211197498 validation 3.5663335992820886\n",
      "epoch: 385 test error: 3.362497483757189\n",
      "epoch:  390 loss 0.00035340522 train 1.1720597930814576 validation 3.5620027997462627\n",
      "epoch: 390 test error: 3.3625716862999226\n",
      "epoch:  395 loss 0.00034774118 train 1.1618366757869485 validation 3.5587560584347853\n",
      "epoch: 395 test error: 3.3623198853423384\n",
      "epoch:  400 loss 0.00034216655 train 1.151290086203547 validation 3.5571808041247595\n",
      "epoch: 400 test error: 3.3628171292627873\n",
      "epoch:  405 loss 0.00033672547 train 1.1419916778141033 validation 3.551076130100786\n",
      "epoch: 405 test error: 3.362803921759446\n",
      "epoch:  410 loss 0.00033141882 train 1.132220828883414 validation 3.54837990232612\n",
      "epoch: 410 test error: 3.3632325067777793\n",
      "epoch:  415 loss 0.00032613327 train 1.1224415074314071 validation 3.546550060346697\n",
      "epoch: 415 test error: 3.3639531244613274\n",
      "epoch:  420 loss 0.0003210111 train 1.1135140577026417 validation 3.541711652963106\n",
      "epoch: 420 test error: 3.36400642391888\n",
      "epoch:  425 loss 0.0003159863 train 1.1037075483022878 validation 3.5395852573747653\n",
      "epoch: 425 test error: 3.3647957355336153\n",
      "epoch:  430 loss 0.0003110667 train 1.0947741413406993 validation 3.5366472110316236\n",
      "epoch: 430 test error: 3.365721219985096\n",
      "epoch:  435 loss 0.00030628382 train 1.0853424697130425 validation 3.530821422051819\n",
      "epoch: 435 test error: 3.3654107697414744\n",
      "epoch:  440 loss 0.0003016074 train 1.0765424913145991 validation 3.529254208198878\n",
      "epoch: 440 test error: 3.366482854791844\n",
      "epoch:  445 loss 0.0002970093 train 1.0673590830171817 validation 3.5247136133591597\n",
      "epoch: 445 test error: 3.3670437447242696\n",
      "epoch:  450 loss 0.00029247906 train 1.0589706016845328 validation 3.523311510090236\n",
      "epoch: 450 test error: 3.368426634723516\n",
      "epoch:  455 loss 0.00028805027 train 1.0501806163660343 validation 3.519941151269262\n",
      "epoch: 455 test error: 3.3696238768061852\n",
      "epoch:  460 loss 0.00028372178 train 1.0414332828131874 validation 3.515585933772194\n",
      "epoch: 460 test error: 3.3704506399727316\n",
      "epoch:  465 loss 0.0002794773 train 1.0333255872476643 validation 3.514629516533697\n",
      "epoch: 465 test error: 3.3722188100350965\n",
      "epoch:  470 loss 0.00027533164 train 1.0248990280025636 validation 3.5122678871715403\n",
      "epoch: 470 test error: 3.3738512697000744\n",
      "epoch:  475 loss 0.00027124493 train 1.016824161096635 validation 3.509508432567132\n",
      "epoch: 475 test error: 3.375322694632878\n",
      "epoch:  480 loss 0.00026722564 train 1.0087232293269066 validation 3.507426591317874\n",
      "epoch: 480 test error: 3.377447118985641\n",
      "epoch:  485 loss 0.00026330014 train 1.0005391338158278 validation 3.505431554925812\n",
      "epoch: 485 test error: 3.379439103117534\n",
      "epoch:  490 loss 0.00025942133 train 0.9926254816132295 validation 3.503035572509049\n",
      "epoch: 490 test error: 3.381372451909226\n",
      "epoch:  495 loss 0.0002556276 train 0.9848275274445458 validation 3.5008706628820336\n",
      "epoch: 495 test error: 3.3835947791627543\n",
      "epoch:  500 loss 0.00025190855 train 0.9768652402549268 validation 3.4992371595311385\n",
      "epoch: 500 test error: 3.3861082265063205\n",
      "epoch:  505 loss 0.0002482582 train 0.969088061146965 validation 3.497616477590025\n",
      "epoch: 505 test error: 3.388460913928061\n",
      "epoch:  510 loss 0.00024466167 train 0.961391548047488 validation 3.4960533045113515\n",
      "epoch: 510 test error: 3.391212107858325\n",
      "epoch:  515 loss 0.00024114919 train 0.9538528658483404 validation 3.494514639303544\n",
      "epoch: 515 test error: 3.3938978072936865\n",
      "epoch:  520 loss 0.0002376906 train 0.9463036692442218 validation 3.49309930030279\n",
      "epoch: 520 test error: 3.3966621668147203\n",
      "epoch:  525 loss 0.00023432139 train 0.938845654848372 validation 3.4915103873049684\n",
      "epoch: 525 test error: 3.3994055682507516\n",
      "epoch:  530 loss 0.00023098725 train 0.9314220506370998 validation 3.4902159269095585\n",
      "epoch: 530 test error: 3.4023166471756277\n",
      "epoch:  535 loss 0.00022769839 train 0.9241602127261666 validation 3.48970023048559\n",
      "epoch: 535 test error: 3.4057402148048483\n",
      "epoch:  540 loss 0.00022451619 train 0.9172465707228713 validation 3.488236201650184\n",
      "epoch: 540 test error: 3.4086485528207295\n",
      "epoch:  545 loss 0.00022137443 train 0.9104260430519414 validation 3.4879985023153743\n",
      "epoch: 545 test error: 3.411961365377951\n",
      "epoch:  550 loss 0.0002182886 train 0.9035179948557146 validation 3.487327136160673\n",
      "epoch: 550 test error: 3.415395371117272\n",
      "epoch:  555 loss 0.0002152894 train 0.8968166482233842 validation 3.4865459527092093\n",
      "epoch: 555 test error: 3.4186674739802627\n",
      "epoch:  560 loss 0.0002123192 train 0.8901123255635527 validation 3.48615079064177\n",
      "epoch: 560 test error: 3.422404925080088\n",
      "epoch:  565 loss 0.00020942507 train 0.8836073260048153 validation 3.4860006731724966\n",
      "epoch: 565 test error: 3.425985727237468\n",
      "epoch:  570 loss 0.00020655482 train 0.8770333265388325 validation 3.486007286011267\n",
      "epoch: 570 test error: 3.4299911580843836\n",
      "epoch:  575 loss 0.0002037525 train 0.8705726613557332 validation 3.4859263092792525\n",
      "epoch: 575 test error: 3.433678866246128\n",
      "epoch:  580 loss 0.00020099699 train 0.8640937077434366 validation 3.4859939998315657\n",
      "epoch: 580 test error: 3.437399319893049\n",
      "epoch:  585 loss 0.00019829089 train 0.8575714207788845 validation 3.4871883964497523\n",
      "epoch: 585 test error: 3.4415467154443924\n",
      "epoch:  590 loss 0.00019565642 train 0.8514754101286331 validation 3.488038206065756\n",
      "epoch: 590 test error: 3.445379605110573\n",
      "epoch:  595 loss 0.00019305952 train 0.8452329100442342 validation 3.489526653802077\n",
      "epoch: 595 test error: 3.4497350193178664\n",
      "epoch:  600 loss 0.00019050729 train 0.8391185219502402 validation 3.4904494613894923\n",
      "epoch: 600 test error: 3.453535082578057\n",
      "epoch:  605 loss 0.0001880216 train 0.8330777659547278 validation 3.490648675166028\n",
      "epoch: 605 test error: 3.457554601860527\n",
      "epoch:  610 loss 0.00018552678 train 0.8269388255766369 validation 3.492795158813758\n",
      "epoch: 610 test error: 3.461981404300884\n",
      "epoch:  615 loss 0.0001831287 train 0.8210914526000852 validation 3.494823581137432\n",
      "epoch: 615 test error: 3.466303087106774\n",
      "epoch:  620 loss 0.00018075811 train 0.8148730427637537 validation 3.495889622669523\n",
      "epoch: 620 test error: 3.4703579259051542\n",
      "epoch:  625 loss 0.00017842685 train 0.8089483286240582 validation 3.49639289238601\n",
      "epoch: 625 test error: 3.4743123179113793\n",
      "epoch:  630 loss 0.00017614862 train 0.8033557331289494 validation 3.498377987345358\n",
      "epoch: 630 test error: 3.479101575312986\n",
      "epoch:  635 loss 0.00017390614 train 0.7975070523378736 validation 3.4991770645074136\n",
      "epoch: 635 test error: 3.483359396087203\n",
      "epoch:  640 loss 0.00017175279 train 0.7915912108487042 validation 3.499787869060862\n",
      "epoch: 640 test error: 3.4874241804217516\n",
      "epoch:  645 loss 0.00016959489 train 0.785883442686548 validation 3.501174266953709\n",
      "epoch: 645 test error: 3.4915404992045618\n",
      "epoch:  650 loss 0.0001674864 train 0.780214002464151 validation 3.5012172647111357\n",
      "epoch: 650 test error: 3.495644803737942\n",
      "epoch:  655 loss 0.00016543301 train 0.7749706624816955 validation 3.502011751213583\n",
      "epoch: 655 test error: 3.4997622766770538\n",
      "epoch:  660 loss 0.00016340025 train 0.769549346511553 validation 3.5033121148473594\n",
      "epoch: 660 test error: 3.5041737456163577\n",
      "epoch:  665 loss 0.00016140334 train 0.7642139471339886 validation 3.505270146462375\n",
      "epoch: 665 test error: 3.508360515412602\n",
      "epoch:  670 loss 0.00015944587 train 0.7590585823600112 validation 3.5064287967336742\n",
      "epoch: 670 test error: 3.512139399145895\n",
      "epoch:  675 loss 0.00015754875 train 0.7539711074499735 validation 3.5077667256748852\n",
      "epoch: 675 test error: 3.5159075273396656\n",
      "epoch:  680 loss 0.0001556592 train 0.7487219253661005 validation 3.5105300647815842\n",
      "epoch: 680 test error: 3.520432009734342\n",
      "epoch:  685 loss 0.00015379324 train 0.7434994668988457 validation 3.5112574137529715\n",
      "epoch: 685 test error: 3.5239907831987933\n",
      "epoch:  690 loss 0.00015197683 train 0.7386657164113969 validation 3.512725678796033\n",
      "epoch: 690 test error: 3.5279810106008944\n",
      "epoch:  695 loss 0.00015018377 train 0.733656990302352 validation 3.5146424650481554\n",
      "epoch: 695 test error: 3.5317941540972204\n",
      "epoch:  700 loss 0.0001484481 train 0.7287956059020758 validation 3.516455596551005\n",
      "epoch: 700 test error: 3.535803574998771\n",
      "epoch:  705 loss 0.00014672229 train 0.7241267454553264 validation 3.5183753668341264\n",
      "epoch: 705 test error: 3.5400639012567248\n",
      "epoch:  710 loss 0.00014502638 train 0.7192848071603184 validation 3.520136713031253\n",
      "epoch: 710 test error: 3.543678851592746\n",
      "epoch:  715 loss 0.00014338513 train 0.7146939626686603 validation 3.5214480708049694\n",
      "epoch: 715 test error: 3.547540658487092\n",
      "epoch:  720 loss 0.00014175284 train 0.7097258347285929 validation 3.5230491466909304\n",
      "epoch: 720 test error: 3.5510333207481213\n",
      "epoch:  725 loss 0.00014013513 train 0.7052436711232603 validation 3.525512561173898\n",
      "epoch: 725 test error: 3.5554266379905664\n",
      "epoch:  730 loss 0.00013854764 train 0.7010755320688171 validation 3.5271157841602907\n",
      "epoch: 730 test error: 3.5593026970040906\n",
      "epoch:  735 loss 0.0001370021 train 0.6965807609890604 validation 3.5292454963714035\n",
      "epoch: 735 test error: 3.5631558943293355\n",
      "epoch:  740 loss 0.00013548852 train 0.6925157894274389 validation 3.530303052657605\n",
      "epoch: 740 test error: 3.5666197335622143\n",
      "epoch:  745 loss 0.00013405645 train 0.6873964705101078 validation 3.533691142200378\n",
      "epoch: 745 test error: 3.570208709185255\n",
      "epoch:  750 loss 0.0001335431 train 0.6973919658728573 validation 3.5314498435502917\n",
      "epoch: 750 test error: 3.577475365106561\n",
      "epoch:  755 loss 0.0001547039 train 0.8071498032983996 validation 3.575512371296519\n",
      "epoch: 755 test error: 3.61078608546677\n",
      "epoch:  760 loss 0.00028625267 train 1.2029263529600087 validation 3.583425800326351\n",
      "epoch: 760 test error: 3.6655308863752394\n",
      "epoch:  765 loss 0.00020983379 train 1.0179386389058163 validation 3.5504143306299967\n",
      "epoch: 765 test error: 3.6213726689048515\n",
      "epoch:  770 loss 0.0001602305 train 0.855754240064675 validation 3.529765734135801\n",
      "epoch: 770 test error: 3.5831560059148915\n",
      "epoch:  775 loss 0.0001289268 train 0.6945583820354342 validation 3.5374554590787115\n",
      "epoch: 775 test error: 3.580345136419231\n",
      "epoch:  780 loss 0.00012779434 train 0.6820590713245046 validation 3.5443618509678254\n",
      "epoch: 780 test error: 3.5852664237198515\n",
      "epoch:  785 loss 0.00013210406 train 0.7123190320240541 validation 3.534320509076886\n",
      "epoch: 785 test error: 3.580536441768479\n",
      "epoch:  790 loss 0.00012757134 train 0.68865480991731 validation 3.5279663135473833\n",
      "epoch: 790 test error: 3.578146360092097\n",
      "epoch:  795 loss 0.0001221434 train 0.657230028542736 validation 3.528724076003242\n",
      "epoch: 795 test error: 3.5814507244060336\n",
      "epoch:  800 loss 0.000119998804 train 0.6458879858142861 validation 3.5289215313737703\n",
      "epoch: 800 test error: 3.585896180360578\n",
      "epoch:  805 loss 0.0001191817 train 0.6489835679488261 validation 3.530924370682217\n",
      "epoch: 805 test error: 3.5887684247073954\n",
      "epoch:  810 loss 0.00011813693 train 0.6477103932192246 validation 3.534101066672505\n",
      "epoch: 810 test error: 3.5919013939481346\n",
      "epoch:  815 loss 0.00011682254 train 0.6424099870356286 validation 3.5369717573291153\n",
      "epoch: 815 test error: 3.5943575245934762\n",
      "epoch:  820 loss 0.00011548006 train 0.6362974250039499 validation 3.537736218726934\n",
      "epoch: 820 test error: 3.596678411471878\n",
      "epoch:  825 loss 0.00011418737 train 0.6304427996764266 validation 3.539286025872446\n",
      "epoch: 825 test error: 3.5992009661302253\n",
      "epoch:  830 loss 0.0001129851 train 0.6256106863819023 validation 3.5421105788901617\n",
      "epoch: 830 test error: 3.6023660046673416\n",
      "epoch:  835 loss 0.00011183063 train 0.6213892671368704 validation 3.5440065402676812\n",
      "epoch: 835 test error: 3.6050836403352893\n",
      "epoch:  840 loss 0.00011069702 train 0.6170137136903038 validation 3.5459427609346896\n",
      "epoch: 840 test error: 3.6078688713953295\n",
      "epoch:  845 loss 0.000109567074 train 0.6121686329863996 validation 3.5479222421663668\n",
      "epoch: 845 test error: 3.6105828907154565\n",
      "epoch:  850 loss 0.00010847154 train 0.6080269530529321 validation 3.549140819758193\n",
      "epoch: 850 test error: 3.6131956976721495\n",
      "epoch:  855 loss 0.00010739891 train 0.6045894449754377 validation 3.5514916197484467\n",
      "epoch: 855 test error: 3.6159113540622565\n",
      "epoch:  860 loss 0.0001063631 train 0.6008482453249422 validation 3.5535310760754015\n",
      "epoch: 860 test error: 3.618620192801874\n",
      "epoch:  865 loss 0.000105317384 train 0.5971052162298328 validation 3.5555492380168503\n",
      "epoch: 865 test error: 3.621260940672206\n",
      "epoch:  870 loss 0.00010430207 train 0.5933563925707479 validation 3.5569469542778256\n",
      "epoch: 870 test error: 3.6238341652354604\n",
      "epoch:  875 loss 0.00010330012 train 0.5904363254188544 validation 3.559368594041389\n",
      "epoch: 875 test error: 3.6264849401483703\n",
      "epoch:  880 loss 0.00010231828 train 0.5872877006054034 validation 3.5612974014962195\n",
      "epoch: 880 test error: 3.6292241132936467\n",
      "epoch:  885 loss 0.000101329315 train 0.5831277031862474 validation 3.5626498328352487\n",
      "epoch: 885 test error: 3.6315017928143387\n",
      "epoch:  890 loss 0.00010036349 train 0.5803351788139945 validation 3.5651943601236313\n",
      "epoch: 890 test error: 3.634279192531673\n",
      "epoch:  895 loss 9.9416015e-05 train 0.5770098477302817 validation 3.5669250879367063\n",
      "epoch: 895 test error: 3.6367420112188453\n",
      "epoch:  900 loss 9.847621e-05 train 0.5734465519167028 validation 3.568733440345193\n",
      "epoch: 900 test error: 3.638946266267796\n",
      "epoch:  905 loss 9.7554635e-05 train 0.5705500118404184 validation 3.570371983489999\n",
      "epoch: 905 test error: 3.641397889279761\n",
      "epoch:  910 loss 9.6643744e-05 train 0.567132475163931 validation 3.572017397424119\n",
      "epoch: 910 test error: 3.6434880291263294\n",
      "epoch:  915 loss 9.5777425e-05 train 0.5650035599691557 validation 3.5742638315231523\n",
      "epoch: 915 test error: 3.646638739993178\n",
      "epoch:  920 loss 9.525603e-05 train 0.5650859309465678 validation 3.576276827522729\n",
      "epoch: 920 test error: 3.6479972196515784\n",
      "epoch:  925 loss 0.00010045028 train 0.6224038949831898 validation 3.5865001403979773\n",
      "epoch: 925 test error: 3.665157416154241\n",
      "epoch:  930 loss 0.00019972303 train 0.9993663384241125 validation 3.6504439596974527\n",
      "epoch: 930 test error: 3.7663893613241965\n",
      "epoch:  935 loss 9.5180796e-05 train 0.582918418850498 validation 3.59481324979243\n",
      "epoch: 935 test error: 3.6623596494223403\n",
      "epoch:  940 loss 9.303479e-05 train 0.5682864345567218 validation 3.5831990022661317\n",
      "epoch: 940 test error: 3.658869034797428\n",
      "epoch:  945 loss 9.231043e-05 train 0.5603101500928256 validation 3.5784567511686776\n",
      "epoch: 945 test error: 3.660150262963558\n",
      "epoch:  950 loss 9.820362e-05 train 0.6157967715746511 validation 3.5767232803025393\n",
      "epoch: 950 test error: 3.6562968141825665\n",
      "epoch:  955 loss 0.00010009393 train 0.6309322239193621 validation 3.5664081782365122\n",
      "epoch: 955 test error: 3.6479600900093914\n",
      "epoch:  960 loss 9.585885e-05 train 0.6044839778926592 validation 3.563665344518329\n",
      "epoch: 960 test error: 3.643404405415559\n",
      "epoch:  965 loss 9.0716516e-05 train 0.5665204350138376 validation 3.566620887668713\n",
      "epoch: 965 test error: 3.6438113658517137\n",
      "epoch:  970 loss 8.766955e-05 train 0.5396189719628266 validation 3.5680934777423845\n",
      "epoch: 970 test error: 3.6461092101377686\n",
      "epoch:  975 loss 8.633798e-05 train 0.5306276563572659 validation 3.571829868260225\n",
      "epoch: 975 test error: 3.6488936302547663\n",
      "epoch:  980 loss 8.562883e-05 train 0.5293447804619907 validation 3.575750812006383\n",
      "epoch: 980 test error: 3.6516224489379607\n",
      "epoch:  985 loss 8.495106e-05 train 0.5281401956253666 validation 3.5775670220550015\n",
      "epoch: 985 test error: 3.653491599018892\n",
      "epoch:  990 loss 8.421046e-05 train 0.5251466672823126 validation 3.579164251144891\n",
      "epoch: 990 test error: 3.6552885730224074\n",
      "epoch:  995 loss 8.343735e-05 train 0.5214396608388305 validation 3.5810390944027803\n",
      "epoch: 995 test error: 3.6570315470884216\n",
      "epoch:  1000 loss 8.266936e-05 train 0.517775257163403 validation 3.5824631827614506\n",
      "epoch: 1000 test error: 3.6585107208252126\n",
      "epoch:  1005 loss 8.1926606e-05 train 0.5143164672774672 validation 3.5847033937433115\n",
      "epoch: 1005 test error: 3.660709549292607\n",
      "epoch:  1010 loss 8.120883e-05 train 0.5107703846403099 validation 3.586333492877669\n",
      "epoch: 1010 test error: 3.6625429743349507\n",
      "epoch:  1015 loss 8.0504404e-05 train 0.5083028444544355 validation 3.587891301017423\n",
      "epoch: 1015 test error: 3.6644868213935684\n",
      "epoch:  1020 loss 7.981165e-05 train 0.5054684541871869 validation 3.5906366478664196\n",
      "epoch: 1020 test error: 3.666683571884672\n",
      "epoch:  1025 loss 7.913579e-05 train 0.5027511028986145 validation 3.592069124696689\n",
      "epoch: 1025 test error: 3.668460372968184\n",
      "epoch:  1030 loss 7.847456e-05 train 0.5008365079920821 validation 3.59381772981442\n",
      "epoch: 1030 test error: 3.6704417569745895\n",
      "epoch:  1035 loss 7.7799574e-05 train 0.4980266520603171 validation 3.5962984090907977\n",
      "epoch: 1035 test error: 3.672578755894223\n",
      "epoch:  1040 loss 7.714105e-05 train 0.4950681339799549 validation 3.5979590781611024\n",
      "epoch: 1040 test error: 3.6743650651527555\n",
      "epoch:  1045 loss 7.6488206e-05 train 0.49238077564208294 validation 3.5998112125822868\n",
      "epoch: 1045 test error: 3.676254069084325\n",
      "epoch:  1050 loss 7.585547e-05 train 0.49030929402653933 validation 3.6019936008549194\n",
      "epoch: 1050 test error: 3.6783243671608212\n",
      "epoch:  1055 loss 7.521525e-05 train 0.4872447062565993 validation 3.6038702927670068\n",
      "epoch: 1055 test error: 3.6800244731561196\n",
      "epoch:  1060 loss 7.458717e-05 train 0.48496396682580817 validation 3.605477733041569\n",
      "epoch: 1060 test error: 3.68182433765381\n",
      "epoch:  1065 loss 7.396557e-05 train 0.4827925550521326 validation 3.60786727721359\n",
      "epoch: 1065 test error: 3.6839222983152156\n",
      "epoch:  1070 loss 7.3357514e-05 train 0.48011761587929724 validation 3.609632949669633\n",
      "epoch: 1070 test error: 3.6854639219204506\n",
      "epoch:  1075 loss 7.2753566e-05 train 0.47788456119890593 validation 3.61156798575805\n",
      "epoch: 1075 test error: 3.6873927495713885\n",
      "epoch:  1080 loss 7.215603e-05 train 0.47509810726032 validation 3.613247004329311\n",
      "epoch: 1080 test error: 3.6890140356572108\n",
      "epoch:  1085 loss 7.1566785e-05 train 0.4731805648310989 validation 3.6147560869962807\n",
      "epoch: 1085 test error: 3.6908905138658943\n",
      "epoch:  1090 loss 7.0979666e-05 train 0.4706729664918479 validation 3.6169264266029097\n",
      "epoch: 1090 test error: 3.6927248739365255\n",
      "epoch:  1095 loss 7.041157e-05 train 0.46853320766781364 validation 3.6181291870399854\n",
      "epoch: 1095 test error: 3.694262153855855\n",
      "epoch:  1100 loss 6.9848225e-05 train 0.466007879464431 validation 3.620965364771132\n",
      "epoch: 1100 test error: 3.6962038497696392\n",
      "epoch:  1105 loss 6.929229e-05 train 0.4646911646282041 validation 3.6218071820888893\n",
      "epoch: 1105 test error: 3.6975017203941483\n",
      "epoch:  1110 loss 6.889618e-05 train 0.4648745949796376 validation 3.6255619465360884\n",
      "epoch: 1110 test error: 3.7002956122800974\n",
      "epoch:  1115 loss 7.0341455e-05 train 0.4914672830967337 validation 3.6202717611570243\n",
      "epoch: 1115 test error: 3.7012178036954526\n",
      "epoch:  1120 loss 9.956446e-05 train 0.6759165406367585 validation 3.68788663951138\n",
      "epoch: 1120 test error: 3.7618058946982504\n",
      "epoch:  1125 loss 0.00022490819 train 1.0564716048357727 validation 3.659694907152934\n",
      "epoch: 1125 test error: 3.778630024068399\n",
      "epoch:  1130 loss 0.0001295455 train 0.7945780105730041 validation 3.6442736610988593\n",
      "epoch: 1130 test error: 3.749685521133211\n",
      "epoch:  1135 loss 9.973363e-05 train 0.6798137728225597 validation 3.6246537309417874\n",
      "epoch: 1135 test error: 3.723287684815594\n",
      "epoch:  1140 loss 8.824421e-05 train 0.6241128811085397 validation 3.614831495786141\n",
      "epoch: 1140 test error: 3.7068919767360686\n",
      "epoch:  1145 loss 8.010163e-05 train 0.5789864275398654 validation 3.6053374790937944\n",
      "epoch: 1145 test error: 3.695008091330472\n",
      "epoch:  1150 loss 7.341962e-05 train 0.534654400052656 validation 3.602901750587673\n",
      "epoch: 1150 test error: 3.6902932016174335\n",
      "epoch:  1155 loss 6.865962e-05 train 0.49845734834099376 validation 3.6069840486785325\n",
      "epoch: 1155 test error: 3.6894592847808734\n",
      "epoch:  1160 loss 6.5727036e-05 train 0.4709430404435226 validation 3.6096629098600164\n",
      "epoch: 1160 test error: 3.690393064859124\n",
      "epoch:  1165 loss 6.409618e-05 train 0.45375025178456857 validation 3.612837209212257\n",
      "epoch: 1165 test error: 3.6919547359105915\n",
      "epoch:  1170 loss 6.3127016e-05 train 0.4440970576053811 validation 3.6161845130942334\n",
      "epoch: 1170 test error: 3.69401730343635\n",
      "epoch:  1175 loss 6.245784e-05 train 0.43928805783476454 validation 3.617269837079161\n",
      "epoch: 1175 test error: 3.695763713779646\n",
      "epoch:  1180 loss 6.189693e-05 train 0.4357301730966087 validation 3.6190817474069674\n",
      "epoch: 1180 test error: 3.697487531548271\n",
      "epoch:  1185 loss 6.138061e-05 train 0.43306101675745756 validation 3.6201939143213666\n",
      "epoch: 1185 test error: 3.6988176957984327\n",
      "epoch:  1190 loss 6.0900114e-05 train 0.43040263590753947 validation 3.6208832783077414\n",
      "epoch: 1190 test error: 3.7001548563654794\n",
      "epoch:  1195 loss 6.0422208e-05 train 0.4287811311323274 validation 3.6223940365454452\n",
      "epoch: 1195 test error: 3.7017368365052863\n",
      "epoch:  1200 loss 5.9940674e-05 train 0.42629032288634616 validation 3.6231866112598574\n",
      "epoch: 1200 test error: 3.7031709631686995\n",
      "epoch:  1205 loss 5.9456106e-05 train 0.4231992118086384 validation 3.6241945946332303\n",
      "epoch: 1205 test error: 3.7046458703690526\n",
      "epoch:  1210 loss 5.897799e-05 train 0.42135651270029445 validation 3.6260024701273803\n",
      "epoch: 1210 test error: 3.7063551971298483\n",
      "epoch:  1215 loss 5.8512996e-05 train 0.41827695015898164 validation 3.6273301623400562\n",
      "epoch: 1215 test error: 3.708033543657269\n",
      "epoch:  1220 loss 5.806111e-05 train 0.41648058408300337 validation 3.6290888657450813\n",
      "epoch: 1220 test error: 3.7095614534463603\n",
      "epoch:  1225 loss 5.761903e-05 train 0.4151470476406976 validation 3.6312054672824154\n",
      "epoch: 1225 test error: 3.7112059556593744\n",
      "epoch:  1230 loss 5.7182646e-05 train 0.41310374504780434 validation 3.633327496289927\n",
      "epoch: 1230 test error: 3.7130985015883913\n",
      "epoch:  1235 loss 5.6740522e-05 train 0.411024955392051 validation 3.63420871209533\n",
      "epoch: 1235 test error: 3.714522638104416\n",
      "epoch:  1240 loss 5.6309626e-05 train 0.40930446477663585 validation 3.6348783563285885\n",
      "epoch: 1240 test error: 3.7157046303680796\n",
      "epoch:  1245 loss 5.5885277e-05 train 0.4075885549778196 validation 3.636633959966501\n",
      "epoch: 1245 test error: 3.7171604408345282\n",
      "epoch:  1250 loss 5.5464483e-05 train 0.40523884910345764 validation 3.6378817346053522\n",
      "epoch: 1250 test error: 3.7185992640633114\n",
      "epoch:  1255 loss 5.503984e-05 train 0.4035295887875295 validation 3.6386964437045934\n",
      "epoch: 1255 test error: 3.720089012532463\n",
      "epoch:  1260 loss 5.4629738e-05 train 0.40199695053455886 validation 3.6401892839720302\n",
      "epoch: 1260 test error: 3.721423495762544\n",
      "epoch:  1265 loss 5.421543e-05 train 0.4002490757614621 validation 3.6413275530607643\n",
      "epoch: 1265 test error: 3.7227070227581387\n",
      "epoch:  1270 loss 5.3808173e-05 train 0.39880636260201296 validation 3.6422291745305744\n",
      "epoch: 1270 test error: 3.7240875064131553\n",
      "epoch:  1275 loss 5.3405456e-05 train 0.3970147400127384 validation 3.6436846512950263\n",
      "epoch: 1275 test error: 3.7254372082316363\n",
      "epoch:  1280 loss 5.3004267e-05 train 0.39466136937061375 validation 3.6447670181583782\n",
      "epoch: 1280 test error: 3.7267488837655343\n",
      "epoch:  1285 loss 5.260949e-05 train 0.3933016318634641 validation 3.6458152460105246\n",
      "epoch: 1285 test error: 3.728059736627776\n",
      "epoch:  1290 loss 5.2220337e-05 train 0.39186648417804143 validation 3.6464643401053074\n",
      "epoch: 1290 test error: 3.7291710274978023\n",
      "epoch:  1295 loss 5.182297e-05 train 0.3901457385333836 validation 3.647848088298509\n",
      "epoch: 1295 test error: 3.730483430550653\n",
      "epoch:  1300 loss 5.1449675e-05 train 0.38885849270391054 validation 3.6478119438270933\n",
      "epoch: 1300 test error: 3.7313981447968625\n",
      "epoch:  1305 loss 5.1115385e-05 train 0.38784276709128085 validation 3.6510262506275626\n",
      "epoch: 1305 test error: 3.733569485529528\n",
      "epoch:  1310 loss 5.113358e-05 train 0.39730436846031775 validation 3.64640921473571\n",
      "epoch: 1310 test error: 3.7333006077768207\n",
      "epoch:  1315 loss 5.5616518e-05 train 0.45237257242381596 validation 3.6711388406504444\n",
      "epoch: 1315 test error: 3.7476030329410146\n",
      "epoch:  1320 loss 0.00011533374 train 0.7511605370646921 validation 3.6645934836499965\n",
      "epoch: 1320 test error: 3.8076525418554\n",
      "epoch:  1325 loss 0.00013564713 train 0.8122402759689673 validation 3.6944497669839187\n",
      "epoch: 1325 test error: 3.764817328278488\n",
      "epoch:  1330 loss 0.00011324652 train 0.7403422762863686 validation 3.696144322233947\n",
      "epoch: 1330 test error: 3.773512292104967\n",
      "epoch:  1335 loss 8.551194e-05 train 0.6330084288373978 validation 3.676391615732988\n",
      "epoch: 1335 test error: 3.7551749041999507\n",
      "epoch:  1340 loss 6.987864e-05 train 0.5567536003459582 validation 3.6605449794064655\n",
      "epoch: 1340 test error: 3.739755567699356\n",
      "epoch:  1345 loss 6.072006e-05 train 0.5009946462719486 validation 3.64746190879983\n",
      "epoch: 1345 test error: 3.7291709184779265\n",
      "epoch:  1350 loss 5.513651e-05 train 0.46106360107961325 validation 3.6406834307818268\n",
      "epoch: 1350 test error: 3.7244990548696686\n",
      "epoch:  1355 loss 5.169363e-05 train 0.4308729359128925 validation 3.6381703732749635\n",
      "epoch: 1355 test error: 3.72297535028481\n",
      "epoch:  1360 loss 4.9609127e-05 train 0.40993492390235503 validation 3.6360509775863843\n",
      "epoch: 1360 test error: 3.7225706643637\n",
      "epoch:  1365 loss 4.833421e-05 train 0.39599366970889627 validation 3.63699269855504\n",
      "epoch: 1365 test error: 3.72347056684763\n",
      "epoch:  1370 loss 4.7471e-05 train 0.38617842533089525 validation 3.637971743570781\n",
      "epoch: 1370 test error: 3.7245757070973498\n",
      "epoch:  1375 loss 4.6808505e-05 train 0.37940130155596613 validation 3.637960982446692\n",
      "epoch: 1375 test error: 3.726064906823437\n",
      "epoch:  1380 loss 4.6256366e-05 train 0.3734529539379737 validation 3.638734917607039\n",
      "epoch: 1380 test error: 3.7270989645425927\n",
      "epoch:  1385 loss 4.578118e-05 train 0.3688435302863315 validation 3.639723972909723\n",
      "epoch: 1385 test error: 3.7286398435666914\n",
      "epoch:  1390 loss 4.533913e-05 train 0.3645125237735805 validation 3.6401311175025772\n",
      "epoch: 1390 test error: 3.7294557360520244\n",
      "epoch:  1395 loss 4.4938126e-05 train 0.3615509971937479 validation 3.6403576437838012\n",
      "epoch: 1395 test error: 3.730708108857605\n",
      "epoch:  1400 loss 4.457676e-05 train 0.3587430278156288 validation 3.6407744915207223\n",
      "epoch: 1400 test error: 3.7317686541830426\n",
      "epoch:  1405 loss 4.425266e-05 train 0.35762533555845144 validation 3.641031956814976\n",
      "epoch: 1405 test error: 3.733083894238763\n",
      "epoch:  1410 loss 4.3936783e-05 train 0.3562958384878765 validation 3.641518173226663\n",
      "epoch: 1410 test error: 3.7340880819469797\n",
      "epoch:  1415 loss 4.3618285e-05 train 0.3549435383270063 validation 3.64232063059986\n",
      "epoch: 1415 test error: 3.735470583587624\n",
      "epoch:  1420 loss 4.328903e-05 train 0.3538482951554317 validation 3.6435760221754716\n",
      "epoch: 1420 test error: 3.7369724417935926\n",
      "epoch:  1425 loss 4.297481e-05 train 0.3522687744907688 validation 3.645193756617512\n",
      "epoch: 1425 test error: 3.7382977085144904\n",
      "epoch:  1430 loss 4.266452e-05 train 0.35011986324942224 validation 3.645703850069174\n",
      "epoch: 1430 test error: 3.7395256775346244\n",
      "epoch:  1435 loss 4.2352858e-05 train 0.34908181869798477 validation 3.6471517329872283\n",
      "epoch: 1435 test error: 3.740837439104786\n",
      "epoch:  1440 loss 4.2044452e-05 train 0.3480835553480791 validation 3.647938097118711\n",
      "epoch: 1440 test error: 3.7419886222182135\n",
      "epoch:  1445 loss 4.1740685e-05 train 0.3466226236996724 validation 3.648687000706717\n",
      "epoch: 1445 test error: 3.7431043022153014\n",
      "epoch:  1450 loss 4.1445157e-05 train 0.3450651557260636 validation 3.6502177271989273\n",
      "epoch: 1450 test error: 3.7443669652264826\n",
      "epoch:  1455 loss 4.1147945e-05 train 0.34419982504067 validation 3.6507080711372266\n",
      "epoch: 1455 test error: 3.7454873136246354\n",
      "epoch:  1460 loss 4.0847e-05 train 0.3426200736803482 validation 3.651995818066298\n",
      "epoch: 1460 test error: 3.7467450469827863\n",
      "epoch:  1465 loss 4.0554696e-05 train 0.34138062533491104 validation 3.6531184958320093\n",
      "epoch: 1465 test error: 3.74804293976815\n",
      "epoch:  1470 loss 4.0264495e-05 train 0.34023117655180096 validation 3.65331817244882\n",
      "epoch: 1470 test error: 3.7488801212400404\n",
      "epoch:  1475 loss 3.9977524e-05 train 0.33843904793920143 validation 3.654491987798728\n",
      "epoch: 1475 test error: 3.7501334155314217\n",
      "epoch:  1480 loss 3.969041e-05 train 0.337662860824032 validation 3.655736372523904\n",
      "epoch: 1480 test error: 3.7513084816913786\n",
      "epoch:  1485 loss 3.9407085e-05 train 0.3363171607116349 validation 3.656390837745891\n",
      "epoch: 1485 test error: 3.752444779073239\n",
      "epoch:  1490 loss 3.9125684e-05 train 0.33488633109219235 validation 3.6568696197092665\n",
      "epoch: 1490 test error: 3.75334999298944\n",
      "epoch:  1495 loss 3.8850438e-05 train 0.33350370705925164 validation 3.657682326988078\n",
      "epoch: 1495 test error: 3.7545796539808696\n",
      "epoch:  1500 loss 3.857416e-05 train 0.33229729185187956 validation 3.6588848423631224\n",
      "epoch: 1500 test error: 3.7556875760480986\n",
      "epoch:  1505 loss 3.8305745e-05 train 0.3313967682350846 validation 3.659397583233265\n",
      "epoch: 1505 test error: 3.7566355043871815\n",
      "epoch:  1510 loss 3.8043665e-05 train 0.33023050170823676 validation 3.6612529615906357\n",
      "epoch: 1510 test error: 3.758205108347008\n",
      "epoch:  1515 loss 3.788456e-05 train 0.33351067382700983 validation 3.6585983814426726\n",
      "epoch: 1515 test error: 3.758312832751818\n",
      "epoch:  1520 loss 3.927561e-05 train 0.35986730738475026 validation 3.6739824123424296\n",
      "epoch: 1520 test error: 3.7654152596445685\n",
      "epoch:  1525 loss 6.721709e-05 train 0.5631019915875629 validation 3.657730319842483\n",
      "epoch: 1525 test error: 3.796883595115153\n",
      "epoch:  1530 loss 0.0002489822 train 1.080939733345312 validation 3.8536250011091466\n",
      "epoch: 1530 test error: 3.948829669717954\n",
      "epoch:  1535 loss 0.00010004121 train 0.6963407255831594 validation 3.7659501941894082\n",
      "epoch: 1535 test error: 3.864070579855769\n",
      "epoch:  1540 loss 7.4025265e-05 train 0.5923600235646942 validation 3.718882235873734\n",
      "epoch: 1540 test error: 3.8118951554762854\n",
      "epoch:  1545 loss 6.606865e-05 train 0.5563262461171796 validation 3.6873523670746886\n",
      "epoch: 1545 test error: 3.7786747550075233\n",
      "epoch:  1550 loss 5.5910412e-05 train 0.5011756439627008 validation 3.6584331117784012\n",
      "epoch: 1550 test error: 3.7524006734700457\n",
      "epoch:  1555 loss 4.4828255e-05 train 0.4275729293080491 validation 3.637715419664058\n",
      "epoch: 1555 test error: 3.7398901464422436\n",
      "epoch:  1560 loss 3.7818205e-05 train 0.35812369224283486 validation 3.63059084988041\n",
      "epoch: 1560 test error: 3.7367635008906337\n",
      "epoch:  1565 loss 3.5526155e-05 train 0.3210310286866337 validation 3.6278041883226684\n",
      "epoch: 1565 test error: 3.737337653350399\n",
      "epoch:  1570 loss 3.5444566e-05 train 0.3228475493295507 validation 3.629673129864591\n",
      "epoch: 1570 test error: 3.738623079392367\n",
      "epoch:  1575 loss 3.5589026e-05 train 0.33054347187843186 validation 3.632795795097851\n",
      "epoch: 1575 test error: 3.740138325859205\n",
      "epoch:  1580 loss 3.5378445e-05 train 0.33018770493430655 validation 3.63367805791737\n",
      "epoch: 1580 test error: 3.7412442144216107\n",
      "epoch:  1585 loss 3.4957015e-05 train 0.3256301494453109 validation 3.6353456055654294\n",
      "epoch: 1585 test error: 3.7426994593657223\n",
      "epoch:  1590 loss 3.449985e-05 train 0.3202765585949116 validation 3.6369437530617814\n",
      "epoch: 1590 test error: 3.7440349340534427\n",
      "epoch:  1595 loss 3.410317e-05 train 0.3161272464728154 validation 3.637329088467004\n",
      "epoch: 1595 test error: 3.7449793464948136\n",
      "epoch:  1600 loss 3.376693e-05 train 0.31258092555207573 validation 3.6376816982793736\n",
      "epoch: 1600 test error: 3.7460132649187323\n",
      "epoch:  1605 loss 3.3454427e-05 train 0.3100189829406379 validation 3.6384889070557347\n",
      "epoch: 1605 test error: 3.747223999649007\n",
      "epoch:  1610 loss 3.3173466e-05 train 0.30806051147262953 validation 3.639642698863779\n",
      "epoch: 1610 test error: 3.748708682495312\n",
      "epoch:  1615 loss 3.290548e-05 train 0.3062575525124014 validation 3.640801462178496\n",
      "epoch: 1615 test error: 3.749862882549664\n",
      "epoch:  1620 loss 3.2648826e-05 train 0.30431120416565455 validation 3.64155719875709\n",
      "epoch: 1620 test error: 3.7512344592339066\n",
      "epoch:  1625 loss 3.239469e-05 train 0.30227401457336683 validation 3.6418476527162444\n",
      "epoch: 1625 test error: 3.752371496158478\n",
      "epoch:  1630 loss 3.214572e-05 train 0.3014072106377765 validation 3.6425215448497466\n",
      "epoch: 1630 test error: 3.7536143868397884\n",
      "epoch:  1635 loss 3.1903623e-05 train 0.2999622250456289 validation 3.643466266712118\n",
      "epoch: 1635 test error: 3.754830216727103\n",
      "epoch:  1640 loss 3.166371e-05 train 0.29909431857563296 validation 3.6444021592849025\n",
      "epoch: 1640 test error: 3.75614843674463\n",
      "epoch:  1645 loss 3.1429754e-05 train 0.29754052147690063 validation 3.6455214356424026\n",
      "epoch: 1645 test error: 3.7574083419274036\n",
      "epoch:  1650 loss 3.1194068e-05 train 0.29678810441106324 validation 3.6464695374461615\n",
      "epoch: 1650 test error: 3.7587509426144505\n",
      "epoch:  1655 loss 3.0961284e-05 train 0.2953982018979509 validation 3.647443947988046\n",
      "epoch: 1655 test error: 3.759947475043938\n",
      "epoch:  1660 loss 3.0731568e-05 train 0.2937668281049546 validation 3.648169905085135\n",
      "epoch: 1660 test error: 3.7611862098410405\n",
      "epoch:  1665 loss 3.0503761e-05 train 0.2924940782054055 validation 3.6485786832872007\n",
      "epoch: 1665 test error: 3.762241529114764\n",
      "epoch:  1670 loss 3.0279116e-05 train 0.29112040520827664 validation 3.6495743224546295\n",
      "epoch: 1670 test error: 3.763457322207667\n",
      "epoch:  1675 loss 3.005726e-05 train 0.2898957702186007 validation 3.650043762050195\n",
      "epoch: 1675 test error: 3.764517509456315\n",
      "epoch:  1680 loss 2.9833707e-05 train 0.28935897014800027 validation 3.650817954086522\n",
      "epoch: 1680 test error: 3.7657588105568602\n",
      "epoch:  1685 loss 2.9614823e-05 train 0.28791277788950076 validation 3.6512664504584706\n",
      "epoch: 1685 test error: 3.7667326247261186\n",
      "epoch:  1690 loss 2.9398861e-05 train 0.2869793479660245 validation 3.652429599194632\n",
      "epoch: 1690 test error: 3.7680518124654476\n",
      "epoch:  1695 loss 2.9182605e-05 train 0.2861298374150308 validation 3.653300410917199\n",
      "epoch: 1695 test error: 3.76914017030145\n",
      "epoch:  1700 loss 2.8970282e-05 train 0.28419064706571556 validation 3.6532270196973764\n",
      "epoch: 1700 test error: 3.770097769696669\n",
      "epoch:  1705 loss 2.875695e-05 train 0.2827924686695179 validation 3.654137191486973\n",
      "epoch: 1705 test error: 3.771187904707513\n",
      "epoch:  1710 loss 2.8547964e-05 train 0.28190032405747606 validation 3.6547537598047217\n",
      "epoch: 1710 test error: 3.772284483064351\n",
      "epoch:  1715 loss 2.834158e-05 train 0.28071975435892377 validation 3.6551018699554207\n",
      "epoch: 1715 test error: 3.7733307264315257\n",
      "epoch:  1720 loss 2.8132796e-05 train 0.2798633364645077 validation 3.6557868766214456\n",
      "epoch: 1720 test error: 3.7742322859799176\n",
      "epoch:  1725 loss 2.7928092e-05 train 0.2782972058610382 validation 3.6563533455332577\n",
      "epoch: 1725 test error: 3.7753781037896115\n",
      "epoch:  1730 loss 2.7725122e-05 train 0.277572044235963 validation 3.656910762648914\n",
      "epoch: 1730 test error: 3.7763205372737976\n",
      "epoch:  1735 loss 2.7526521e-05 train 0.2766299135157787 validation 3.6572512397074246\n",
      "epoch: 1735 test error: 3.7772878392658926\n",
      "epoch:  1740 loss 2.7327349e-05 train 0.275222005328899 validation 3.657565816256635\n",
      "epoch: 1740 test error: 3.778258734845252\n",
      "epoch:  1745 loss 2.7131091e-05 train 0.2748419326310055 validation 3.6590048534085504\n",
      "epoch: 1745 test error: 3.7794078261285198\n",
      "epoch:  1750 loss 2.6945962e-05 train 0.2739092194187813 validation 3.6587127757322886\n",
      "epoch: 1750 test error: 3.7801165855703682\n",
      "epoch:  1755 loss 2.6802147e-05 train 0.27388777283856897 validation 3.6621003566888293\n",
      "epoch: 1755 test error: 3.7819517606436888\n",
      "epoch:  1760 loss 2.7014426e-05 train 0.2852678151010294 validation 3.65704261019433\n",
      "epoch: 1760 test error: 3.7820129175318944\n",
      "epoch:  1765 loss 3.084537e-05 train 0.34060324333807707 validation 3.6810478712967614\n",
      "epoch: 1765 test error: 3.794160829763727\n",
      "epoch:  1770 loss 7.336589e-05 train 0.5975563155808146 validation 3.662958329069261\n",
      "epoch: 1770 test error: 3.8355317008996814\n",
      "epoch:  1775 loss 0.00016424635 train 0.882340075703629 validation 3.7669060016416656\n",
      "epoch: 1775 test error: 3.8857873414838195\n",
      "epoch:  1780 loss 7.003178e-05 train 0.5823175037908707 validation 3.735636691823592\n",
      "epoch: 1780 test error: 3.853328271927957\n",
      "epoch:  1785 loss 3.5185403e-05 train 0.384363805844867 validation 3.69458082202715\n",
      "epoch: 1785 test error: 3.8091751302541645\n",
      "epoch:  1790 loss 2.7369906e-05 train 0.3011256487604318 validation 3.6731300607938624\n",
      "epoch: 1790 test error: 3.7881246750944206\n",
      "epoch:  1795 loss 2.5712307e-05 train 0.272527541246825 validation 3.6572379033203624\n",
      "epoch: 1795 test error: 3.7761131687863587\n",
      "epoch:  1800 loss 2.5300526e-05 train 0.26398206580252764 validation 3.648146317536141\n",
      "epoch: 1800 test error: 3.7702726404141877\n",
      "epoch:  1805 loss 2.5193533e-05 train 0.2646424390717628 validation 3.6456371157025513\n",
      "epoch: 1805 test error: 3.7685661582235896\n",
      "epoch:  1810 loss 2.5161744e-05 train 0.2680787511758147 validation 3.644255038265523\n",
      "epoch: 1810 test error: 3.767898134203106\n",
      "epoch:  1815 loss 2.5129932e-05 train 0.27109498189371856 validation 3.6436739217068452\n",
      "epoch: 1815 test error: 3.7685978987199165\n",
      "epoch:  1820 loss 2.5050233e-05 train 0.2724160299941338 validation 3.6440920781539976\n",
      "epoch: 1820 test error: 3.7695137268865744\n",
      "epoch:  1825 loss 2.4889674e-05 train 0.27181909112807234 validation 3.6444189040426886\n",
      "epoch: 1825 test error: 3.770970519885822\n",
      "epoch:  1830 loss 2.4622557e-05 train 0.2685625924717431 validation 3.644959670612956\n",
      "epoch: 1830 test error: 3.772261037001935\n",
      "epoch:  1835 loss 2.4290197e-05 train 0.26317455316682065 validation 3.645880430612881\n",
      "epoch: 1835 test error: 3.7736335657106546\n",
      "epoch:  1840 loss 2.3981856e-05 train 0.2576441501866355 validation 3.6468092363215248\n",
      "epoch: 1840 test error: 3.775068267674556\n",
      "epoch:  1845 loss 2.3753415e-05 train 0.2534537973664706 validation 3.6486219380437332\n",
      "epoch: 1845 test error: 3.776374109310954\n",
      "epoch:  1850 loss 2.3592347e-05 train 0.25348719904722244 validation 3.650218440427966\n",
      "epoch: 1850 test error: 3.778200273069737\n",
      "epoch:  1855 loss 2.3446768e-05 train 0.253176117752079 validation 3.651213113214025\n",
      "epoch: 1855 test error: 3.7795619072362125\n",
      "epoch:  1860 loss 2.3272827e-05 train 0.2510857164550018 validation 3.6523723295862887\n",
      "epoch: 1860 test error: 3.781009363185428\n",
      "epoch:  1865 loss 2.3095557e-05 train 0.24963787995761527 validation 3.653487780793798\n",
      "epoch: 1865 test error: 3.7824693470556157\n",
      "epoch:  1870 loss 2.2939877e-05 train 0.24868513556166238 validation 3.6541327363711185\n",
      "epoch: 1870 test error: 3.7836132997121124\n",
      "epoch:  1875 loss 2.2783874e-05 train 0.2474589600281584 validation 3.654795271166301\n",
      "epoch: 1875 test error: 3.784919935545782\n",
      "epoch:  1880 loss 2.2621778e-05 train 0.24691324310595722 validation 3.6557957924514906\n",
      "epoch: 1880 test error: 3.7862155539808096\n",
      "epoch:  1885 loss 2.2465882e-05 train 0.24548774096613513 validation 3.6573456225784473\n",
      "epoch: 1885 test error: 3.787748370268078\n",
      "epoch:  1890 loss 2.2310262e-05 train 0.2446173398726349 validation 3.6582161299160645\n",
      "epoch: 1890 test error: 3.7888115322563958\n",
      "epoch:  1895 loss 2.2154876e-05 train 0.244310563405059 validation 3.658870973186737\n",
      "epoch: 1895 test error: 3.7901851086262797\n",
      "epoch:  1900 loss 2.2002481e-05 train 0.24334860805690178 validation 3.6596382296154037\n",
      "epoch: 1900 test error: 3.7911199917607674\n",
      "epoch:  1905 loss 2.1851261e-05 train 0.24212699652278027 validation 3.6606891574901423\n",
      "epoch: 1905 test error: 3.7926429887875255\n",
      "epoch:  1910 loss 2.1701484e-05 train 0.24044829064921255 validation 3.661791487112351\n",
      "epoch: 1910 test error: 3.793618159669453\n",
      "epoch:  1915 loss 2.15519e-05 train 0.23985652049969283 validation 3.6623812375182556\n",
      "epoch: 1915 test error: 3.7948046649491847\n",
      "epoch:  1920 loss 2.1404052e-05 train 0.23862432549300447 validation 3.6633112616286083\n",
      "epoch: 1920 test error: 3.7959155401397253\n",
      "epoch:  1925 loss 2.1258944e-05 train 0.23841807881007707 validation 3.664404828313678\n",
      "epoch: 1925 test error: 3.797196797928838\n",
      "epoch:  1930 loss 2.1112804e-05 train 0.23779737030882017 validation 3.6649155787962395\n",
      "epoch: 1930 test error: 3.7982048587007284\n",
      "epoch:  1935 loss 2.0969765e-05 train 0.2370133179819713 validation 3.6661096994955797\n",
      "epoch: 1935 test error: 3.7993693112495186\n",
      "epoch:  1940 loss 2.0832003e-05 train 0.23643461583275008 validation 3.6661306976238883\n",
      "epoch: 1940 test error: 3.8001038048752425\n",
      "epoch:  1945 loss 2.0705087e-05 train 0.23556516548840353 validation 3.667561584590306\n",
      "epoch: 1945 test error: 3.8016296536205103\n",
      "epoch:  1950 loss 2.0638377e-05 train 0.238896133847457 validation 3.6667724573690545\n",
      "epoch: 1950 test error: 3.8020578654739094\n",
      "epoch:  1955 loss 2.0990577e-05 train 0.24984412772241693 validation 3.6739388681930762\n",
      "epoch: 1955 test error: 3.8059129601696253\n",
      "epoch:  1960 loss 2.4875862e-05 train 0.3113883604815367 validation 3.6611670099840183\n",
      "epoch: 1960 test error: 3.806436126808676\n",
      "epoch:  1965 loss 5.842086e-05 train 0.5306189197903425 validation 3.7412854859053386\n",
      "epoch: 1965 test error: 3.8708878249652137\n",
      "epoch:  1970 loss 0.00014598308 train 0.8318297368433878 validation 3.6799095621350135\n",
      "epoch: 1970 test error: 3.876846640848466\n",
      "epoch:  1975 loss 3.2002466e-05 train 0.37788781497851426 validation 3.6614304323039852\n",
      "epoch: 1975 test error: 3.8244745766557373\n",
      "epoch:  1980 loss 2.1864857e-05 train 0.27254486719372745 validation 3.6555544560678164\n",
      "epoch: 1980 test error: 3.7970300066399876\n",
      "epoch:  1985 loss 2.9550356e-05 train 0.3583852904623377 validation 3.663082109669126\n",
      "epoch: 1985 test error: 3.795269810020045\n",
      "epoch:  1990 loss 3.022335e-05 train 0.36377115631817925 validation 3.6664115606016945\n",
      "epoch: 1990 test error: 3.7970057456442863\n",
      "epoch:  1995 loss 2.6600594e-05 train 0.3327082566798991 validation 3.665532721463519\n",
      "epoch: 1995 test error: 3.797197264709807\n",
      "epoch:  2000 loss 2.2533306e-05 train 0.28812568371312447 validation 3.6635470791801894\n",
      "epoch: 2000 test error: 3.7955910407724893\n",
      "epoch:  2005 loss 1.990648e-05 train 0.24665548210371474 validation 3.660449171956328\n",
      "epoch: 2005 test error: 3.7933896580124777\n",
      "epoch:  2010 loss 1.9059731e-05 train 0.22505875660822844 validation 3.6572553046161453\n",
      "epoch: 2010 test error: 3.7921967005416968\n",
      "epoch:  2015 loss 1.920531e-05 train 0.2341523903038507 validation 3.6552180424914313\n",
      "epoch: 2015 test error: 3.79217310603263\n",
      "epoch:  2020 loss 1.9321915e-05 train 0.23998648221829064 validation 3.6553209444865047\n",
      "epoch: 2020 test error: 3.7933901162782075\n",
      "epoch:  2025 loss 1.8967776e-05 train 0.233257834133553 validation 3.6552454152318514\n",
      "epoch: 2025 test error: 3.7944372790169205\n",
      "epoch:  2030 loss 1.855301e-05 train 0.22244325507850604 validation 3.657738263479877\n",
      "epoch: 2030 test error: 3.795994253335737\n",
      "epoch:  2035 loss 1.8437011e-05 train 0.22246155149848065 validation 3.6597277653390154\n",
      "epoch: 2035 test error: 3.7977048105218123\n",
      "epoch:  2040 loss 1.835145e-05 train 0.22403429405148476 validation 3.6615172946575116\n",
      "epoch: 2040 test error: 3.79922335742295\n",
      "epoch:  2045 loss 1.8152641e-05 train 0.21966246110593754 validation 3.6624422216596177\n",
      "epoch: 2045 test error: 3.8004664019617445\n",
      "epoch:  2050 loss 1.8024451e-05 train 0.21860764869208826 validation 3.662743170625159\n",
      "epoch: 2050 test error: 3.8013669878308978\n",
      "epoch:  2055 loss 1.7910568e-05 train 0.21759921877487548 validation 3.6627230287312487\n",
      "epoch: 2055 test error: 3.802478423987223\n",
      "epoch:  2060 loss 1.7765698e-05 train 0.2163024706436498 validation 3.664680337929463\n",
      "epoch: 2060 test error: 3.803903408133789\n",
      "epoch:  2065 loss 1.7654424e-05 train 0.21607722983138553 validation 3.665987486259127\n",
      "epoch: 2065 test error: 3.8053445155638452\n",
      "epoch:  2070 loss 1.7521146e-05 train 0.21477353056640197 validation 3.6663366626739253\n",
      "epoch: 2070 test error: 3.806311693540864\n",
      "epoch:  2075 loss 1.7405335e-05 train 0.21431494065038095 validation 3.666846946700844\n",
      "epoch: 2075 test error: 3.8073947850759144\n",
      "epoch:  2080 loss 1.7285356e-05 train 0.2134694550896164 validation 3.668539930170709\n",
      "epoch: 2080 test error: 3.808641183410779\n",
      "epoch:  2085 loss 1.716324e-05 train 0.2126902769365017 validation 3.6688873332020226\n",
      "epoch: 2085 test error: 3.809720410372127\n",
      "epoch:  2090 loss 1.7048318e-05 train 0.21107872764600355 validation 3.6692245093051317\n",
      "epoch: 2090 test error: 3.8106741489622173\n",
      "epoch:  2095 loss 1.6933633e-05 train 0.2110374700027535 validation 3.670405203646169\n",
      "epoch: 2095 test error: 3.811800364844843\n",
      "epoch:  2100 loss 1.6815107e-05 train 0.21032367156989082 validation 3.671312807654569\n",
      "epoch: 2100 test error: 3.8128439924545754\n",
      "epoch:  2105 loss 1.6698536e-05 train 0.20907737465802378 validation 3.671751819402541\n",
      "epoch: 2105 test error: 3.813948047344337\n",
      "epoch:  2110 loss 1.6585605e-05 train 0.20842551159125908 validation 3.6727783212952887\n",
      "epoch: 2110 test error: 3.814835855306222\n",
      "epoch:  2115 loss 1.6471806e-05 train 0.20772905989293788 validation 3.6731748877463133\n",
      "epoch: 2115 test error: 3.815960221836687\n",
      "epoch:  2120 loss 1.636512e-05 train 0.20757718307336154 validation 3.6746254384070287\n",
      "epoch: 2120 test error: 3.816982564347122\n",
      "epoch:  2125 loss 1.6287646e-05 train 0.2081199083611665 validation 3.673574616412858\n",
      "epoch: 2125 test error: 3.8173871207693337\n",
      "epoch:  2130 loss 1.6437341e-05 train 0.2157921043973426 validation 3.6788130716482557\n",
      "epoch: 2130 test error: 3.8203007286410995\n",
      "epoch:  2135 loss 1.8657518e-05 train 0.26167080910622736 validation 3.6687742679044404\n",
      "epoch: 2135 test error: 3.819916654598257\n",
      "epoch:  2140 loss 4.1696665e-05 train 0.4466831921477525 validation 3.731207619735876\n",
      "epoch: 2140 test error: 3.8708015295150755\n",
      "epoch:  2145 loss 0.00016614672 train 0.8824704406653706 validation 3.697570133060552\n",
      "epoch: 2145 test error: 3.9325815073379635\n",
      "epoch:  2150 loss 1.6032262e-05 train 0.2116991079089783 validation 3.6676498472401966\n",
      "epoch: 2150 test error: 3.826315012499419\n",
      "epoch:  2155 loss 3.0374633e-05 train 0.37364352812279245 validation 3.6670202760594837\n",
      "epoch: 2155 test error: 3.8117010455227676\n",
      "epoch:  2160 loss 3.4396795e-05 train 0.40286876044844927 validation 3.6728204575482346\n",
      "epoch: 2160 test error: 3.813178081493052\n",
      "epoch:  2165 loss 3.0237965e-05 train 0.3733731177364359 validation 3.6719914480813816\n",
      "epoch: 2165 test error: 3.8106205039100374\n",
      "epoch:  2170 loss 2.5156345e-05 train 0.33429574543816787 validation 3.6668868745679877\n",
      "epoch: 2170 test error: 3.8083767484012356\n",
      "epoch:  2175 loss 2.100763e-05 train 0.29417978759662267 validation 3.6669558638091604\n",
      "epoch: 2175 test error: 3.8071379989811343\n",
      "epoch:  2180 loss 1.8092109e-05 train 0.2592841803335592 validation 3.6641080849492202\n",
      "epoch: 2180 test error: 3.8055885299748025\n",
      "epoch:  2185 loss 1.6241389e-05 train 0.22900821492644685 validation 3.6621894696505746\n",
      "epoch: 2185 test error: 3.804148568707665\n",
      "epoch:  2190 loss 1.5260552e-05 train 0.20685724883986153 validation 3.662053458722407\n",
      "epoch: 2190 test error: 3.8041757169372876\n",
      "epoch:  2195 loss 1.49159505e-05 train 0.1971208123083424 validation 3.6600467617980597\n",
      "epoch: 2195 test error: 3.80488481106703\n",
      "epoch:  2200 loss 1.4900553e-05 train 0.2000072131653105 validation 3.6586688090749373\n",
      "epoch: 2200 test error: 3.8050546092481063\n",
      "epoch:  2205 loss 1.49019215e-05 train 0.202918154883716 validation 3.6590431760067883\n",
      "epoch: 2205 test error: 3.8061090696377122\n",
      "epoch:  2210 loss 1.4760479e-05 train 0.2011542871847107 validation 3.658348759174797\n",
      "epoch: 2210 test error: 3.80690433229856\n",
      "epoch:  2215 loss 1.4536216e-05 train 0.1957162977319755 validation 3.6594510376228753\n",
      "epoch: 2215 test error: 3.808078729268217\n",
      "epoch:  2220 loss 1.4381808e-05 train 0.19261423021271856 validation 3.6611779125334283\n",
      "epoch: 2220 test error: 3.8097870698517666\n",
      "epoch:  2225 loss 1.4305974e-05 train 0.19369169101202133 validation 3.6637505514911415\n",
      "epoch: 2225 test error: 3.811466364156413\n",
      "epoch:  2230 loss 1.4196438e-05 train 0.1922667187413675 validation 3.664467572993623\n",
      "epoch: 2230 test error: 3.8128051911332905\n",
      "epoch:  2235 loss 1.4072144e-05 train 0.19032497422863245 validation 3.665221086886412\n",
      "epoch: 2235 test error: 3.813874712924839\n",
      "epoch:  2240 loss 1.3977478e-05 train 0.19018197104542878 validation 3.665387053234392\n",
      "epoch: 2240 test error: 3.8150204137066095\n",
      "epoch:  2245 loss 1.3872043e-05 train 0.18843090383377298 validation 3.66608295830413\n",
      "epoch: 2245 test error: 3.816060479973839\n",
      "epoch:  2250 loss 1.3769819e-05 train 0.1878669856533753 validation 3.6674439992699672\n",
      "epoch: 2250 test error: 3.817360007409497\n",
      "epoch:  2255 loss 1.3672281e-05 train 0.18751903953103952 validation 3.668506852352782\n",
      "epoch: 2255 test error: 3.818555229236278\n",
      "epoch:  2260 loss 1.357268e-05 train 0.18655749353631934 validation 3.669068696885422\n",
      "epoch: 2260 test error: 3.819555735485809\n",
      "epoch:  2265 loss 1.34756165e-05 train 0.18559609252418396 validation 3.6698004772782475\n",
      "epoch: 2265 test error: 3.820686911457877\n",
      "epoch:  2270 loss 1.3378986e-05 train 0.1843504649800939 validation 3.6710952815550244\n",
      "epoch: 2270 test error: 3.8218954257742292\n",
      "epoch:  2275 loss 1.3283166e-05 train 0.18423641142590105 validation 3.671565156151895\n",
      "epoch: 2275 test error: 3.8228249369704566\n",
      "epoch:  2280 loss 1.318863e-05 train 0.18352480798681453 validation 3.6726472668379877\n",
      "epoch: 2280 test error: 3.8240197710145862\n",
      "epoch:  2285 loss 1.309375e-05 train 0.1824944481696925 validation 3.6735305597126904\n",
      "epoch: 2285 test error: 3.8249012310770567\n",
      "epoch:  2290 loss 1.299962e-05 train 0.1817930996286805 validation 3.674300624641626\n",
      "epoch: 2290 test error: 3.8261507214633483\n",
      "epoch:  2295 loss 1.2908746e-05 train 0.1807665773726097 validation 3.6748009552585166\n",
      "epoch: 2295 test error: 3.8268773484305147\n",
      "epoch:  2300 loss 1.2819481e-05 train 0.18018313614430223 validation 3.6762056750092773\n",
      "epoch: 2300 test error: 3.8280737060294423\n",
      "epoch:  2305 loss 1.2737896e-05 train 0.1799283046555834 validation 3.6761087468193807\n",
      "epoch: 2305 test error: 3.8288353913893407\n",
      "epoch:  2310 loss 1.267924e-05 train 0.1815498751808594 validation 3.6784328752400013\n",
      "epoch: 2310 test error: 3.8303317075137313\n",
      "epoch:  2315 loss 1.2752914e-05 train 0.1880164143491764 validation 3.675640275709999\n",
      "epoch: 2315 test error: 3.830137294234177\n",
      "epoch:  2320 loss 1.3612605e-05 train 0.2096554533748546 validation 3.6865653773498046\n",
      "epoch: 2320 test error: 3.8356795910109196\n",
      "epoch:  2325 loss 1.999266e-05 train 0.2981934378452158 validation 3.668033736330937\n",
      "epoch: 2325 test error: 3.836204995301444\n",
      "epoch:  2330 loss 6.216572e-05 train 0.548287649332569 validation 3.761848564734213\n",
      "epoch: 2330 test error: 3.916245935688366\n",
      "epoch:  2335 loss 0.00011786406 train 0.7517550091851717 validation 3.677417629870928\n",
      "epoch: 2335 test error: 3.882642635656243\n",
      "epoch:  2340 loss 2.4432755e-05 train 0.3385511704804409 validation 3.6641645449210523\n",
      "epoch: 2340 test error: 3.8470131508479506\n",
      "epoch:  2345 loss 1.6967904e-05 train 0.2629202573720384 validation 3.667426534620929\n",
      "epoch: 2345 test error: 3.824003395890029\n",
      "epoch:  2350 loss 2.7555572e-05 train 0.360731669427633 validation 3.686169175307616\n",
      "epoch: 2350 test error: 3.83257462455592\n",
      "epoch:  2355 loss 2.1374914e-05 train 0.3103540283940481 validation 3.6848341226249115\n",
      "epoch: 2355 test error: 3.832957013731764\n",
      "epoch:  2360 loss 1.3331057e-05 train 0.21236876910087193 validation 3.6750164930983305\n",
      "epoch: 2360 test error: 3.82408576004537\n",
      "epoch:  2365 loss 1.2054417e-05 train 0.18270714577746214 validation 3.6653555610622073\n",
      "epoch: 2365 test error: 3.8189712435111365\n",
      "epoch:  2370 loss 1.3501697e-05 train 0.21884894573892974 validation 3.6591991991555632\n",
      "epoch: 2370 test error: 3.817348737988222\n",
      "epoch:  2375 loss 1.2846187e-05 train 0.20764872639953552 validation 3.659108390728844\n",
      "epoch: 2375 test error: 3.8192890009336806\n",
      "epoch:  2380 loss 1.1540868e-05 train 0.17375288963313412 validation 3.663595253401768\n",
      "epoch: 2380 test error: 3.821175774926872\n",
      "epoch:  2385 loss 1.1610685e-05 train 0.17967982870166208 validation 3.6680983186982927\n",
      "epoch: 2385 test error: 3.823803401348138\n",
      "epoch:  2390 loss 1.1589572e-05 train 0.18082370540948473 validation 3.6710213488653904\n",
      "epoch: 2390 test error: 3.8259663165894877\n",
      "epoch:  2395 loss 1.1191848e-05 train 0.16822643865154788 validation 3.6697761319455706\n",
      "epoch: 2395 test error: 3.826092322286255\n",
      "epoch:  2400 loss 1.1201731e-05 train 0.17230955050863025 validation 3.6676230063065884\n",
      "epoch: 2400 test error: 3.825998298218678\n",
      "epoch:  2405 loss 1.1033541e-05 train 0.16785315470762144 validation 3.670319647196836\n",
      "epoch: 2405 test error: 3.827306923104058\n",
      "epoch:  2410 loss 1.0949742e-05 train 0.16739402831487626 validation 3.6722864820227987\n",
      "epoch: 2410 test error: 3.828948011942466\n",
      "epoch:  2415 loss 1.0847095e-05 train 0.16574333756412654 validation 3.673978841369969\n",
      "epoch: 2415 test error: 3.830179407155776\n",
      "epoch:  2420 loss 1.0766129e-05 train 0.16501916423833124 validation 3.6743816345598592\n",
      "epoch: 2420 test error: 3.8308920539048583\n",
      "epoch:  2425 loss 1.0668789e-05 train 0.16241942575051296 validation 3.6752966075194387\n",
      "epoch: 2425 test error: 3.8321226752023154\n",
      "epoch:  2430 loss 1.0592811e-05 train 0.1629613086846476 validation 3.6770803077164698\n",
      "epoch: 2430 test error: 3.8333065518005403\n",
      "epoch:  2435 loss 1.0508434e-05 train 0.1614138430778352 validation 3.677397793173908\n",
      "epoch: 2435 test error: 3.8341449852231326\n",
      "epoch:  2440 loss 1.0428152e-05 train 0.16099633994340012 validation 3.677893397384127\n",
      "epoch: 2440 test error: 3.835082299505535\n",
      "epoch:  2445 loss 1.0351261e-05 train 0.16002961915771685 validation 3.6791059235939527\n",
      "epoch: 2445 test error: 3.8363068134288683\n",
      "epoch:  2450 loss 1.0275568e-05 train 0.1596697283000998 validation 3.67891738355562\n",
      "epoch: 2450 test error: 3.836891908326857\n",
      "epoch:  2455 loss 1.0208486e-05 train 0.1599604478652244 validation 3.6808842176669834\n",
      "epoch: 2455 test error: 3.8384210351432375\n",
      "epoch:  2460 loss 1.0167648e-05 train 0.16162054303703377 validation 3.6792925184755547\n",
      "epoch: 2460 test error: 3.838585295774733\n",
      "epoch:  2465 loss 1.02365375e-05 train 0.16649865987948667 validation 3.6842045790845583\n",
      "epoch: 2465 test error: 3.8411545078795175\n",
      "epoch:  2470 loss 1.0939731e-05 train 0.19031800849712424 validation 3.6765659327729363\n",
      "epoch: 2470 test error: 3.839926201909684\n",
      "epoch:  2475 loss 1.5773065e-05 train 0.261165028821968 validation 3.7031452720120446\n",
      "epoch: 2475 test error: 3.8555067275314903\n",
      "epoch:  2480 loss 4.78272e-05 train 0.4871665969999565 validation 3.6720770828575358\n",
      "epoch: 2480 test error: 3.875618547932874\n",
      "epoch:  2485 loss 0.00012254347 train 0.7686223279839448 validation 3.796135019547815\n",
      "epoch: 2485 test error: 3.963927061984524\n",
      "epoch:  2490 loss 9.889622e-06 train 0.16155147231906472 validation 3.7034995695672652\n",
      "epoch: 2490 test error: 3.855441204629252\n",
      "epoch:  2495 loss 3.410901e-05 train 0.413329919668341 validation 3.661202083160512\n",
      "epoch: 2495 test error: 3.8362329991740016\n",
      "epoch:  2500 loss 2.7862887e-05 train 0.37127469821264847 validation 3.6571225709604325\n",
      "epoch: 2500 test error: 3.838515264537935\n",
      "epoch:  2505 loss 1.1566177e-05 train 0.20693679569974222 validation 3.6564462234229724\n",
      "epoch: 2505 test error: 3.8287775632084706\n",
      "epoch:  2510 loss 1.0637736e-05 train 0.18846042992684436 validation 3.66439359103189\n",
      "epoch: 2510 test error: 3.826708906681384\n",
      "epoch:  2515 loss 1.32729e-05 train 0.2329395548866547 validation 3.675344380068452\n",
      "epoch: 2515 test error: 3.8320845234649505\n",
      "epoch:  2520 loss 1.1135704e-05 train 0.19956425933663843 validation 3.676602240420232\n",
      "epoch: 2520 test error: 3.832735235051381\n",
      "epoch:  2525 loss 9.287262e-06 train 0.15146415599467639 validation 3.6709692145955866\n",
      "epoch: 2525 test error: 3.8303157327731796\n",
      "epoch:  2530 loss 9.925793e-06 train 0.1766433208406237 validation 3.6669600427375646\n",
      "epoch: 2530 test error: 3.829420753374362\n",
      "epoch:  2535 loss 9.582134e-06 train 0.16850727692311104 validation 3.66677843818333\n",
      "epoch: 2535 test error: 3.830725872840033\n",
      "epoch:  2540 loss 9.07882e-06 train 0.14971791755123223 validation 3.6706928986012146\n",
      "epoch: 2540 test error: 3.8323536844506814\n",
      "epoch:  2545 loss 9.239798e-06 train 0.15926357272151057 validation 3.6747455856902604\n",
      "epoch: 2545 test error: 3.834873493297756\n",
      "epoch:  2550 loss 8.928339e-06 train 0.14879185718985036 validation 3.673243698764926\n",
      "epoch: 2550 test error: 3.834973627118631\n",
      "epoch:  2555 loss 8.944151e-06 train 0.1523060543415488 validation 3.671586973892155\n",
      "epoch: 2555 test error: 3.8353043484140787\n",
      "epoch:  2560 loss 8.779873e-06 train 0.14601906134814244 validation 3.6730123257616354\n",
      "epoch: 2560 test error: 3.836522528763159\n",
      "epoch:  2565 loss 8.756768e-06 train 0.14853688861611686 validation 3.675810901559907\n",
      "epoch: 2565 test error: 3.838641610447914\n",
      "epoch:  2570 loss 8.640951e-06 train 0.1457060174925469 validation 3.6752148988373596\n",
      "epoch: 2570 test error: 3.839122521788688\n",
      "epoch:  2575 loss 8.578838e-06 train 0.14416282582681803 validation 3.675510879713153\n",
      "epoch: 2575 test error: 3.839961634293322\n",
      "epoch:  2580 loss 8.520807e-06 train 0.14489061188917796 validation 3.6775120725384016\n",
      "epoch: 2580 test error: 3.8416839672959053\n",
      "epoch:  2585 loss 8.438085e-06 train 0.14301849192149071 validation 3.6769983640168658\n",
      "epoch: 2585 test error: 3.8421262008895205\n",
      "epoch:  2590 loss 8.366887e-06 train 0.14275541842985717 validation 3.678268010140653\n",
      "epoch: 2590 test error: 3.8433466492464015\n",
      "epoch:  2595 loss 8.301794e-06 train 0.14196549680169573 validation 3.6788799936737497\n",
      "epoch: 2595 test error: 3.8443147302394354\n",
      "epoch:  2600 loss 8.242395e-06 train 0.1418011991164703 validation 3.6789871594964776\n",
      "epoch: 2600 test error: 3.8451710626463362\n",
      "epoch:  2605 loss 8.190701e-06 train 0.14175767985558405 validation 3.6809734507017184\n",
      "epoch: 2605 test error: 3.8465969266124604\n",
      "epoch:  2610 loss 8.196979e-06 train 0.14581628570057267 validation 3.678979818722992\n",
      "epoch: 2610 test error: 3.846529269161975\n",
      "epoch:  2615 loss 8.56808e-06 train 0.1597412208616921 validation 3.6868218225525515\n",
      "epoch: 2615 test error: 3.8507885216138487\n",
      "epoch:  2620 loss 1.1642501e-05 train 0.22258569613157553 validation 3.6720720981983472\n",
      "epoch: 2620 test error: 3.8486935111089773\n",
      "epoch:  2625 loss 3.5727575e-05 train 0.42067003168399564 validation 3.742399137710151\n",
      "epoch: 2625 test error: 3.904438504394371\n",
      "epoch:  2630 loss 0.00014249852 train 0.8343432170394669 validation 3.692644042433926\n",
      "epoch: 2630 test error: 3.951380928097045\n",
      "epoch:  2635 loss 1.7255265e-05 train 0.28409198724948165 validation 3.6785777290233184\n",
      "epoch: 2635 test error: 3.8453365903660077\n",
      "epoch:  2640 loss 5.1334646e-05 train 0.5080023623992631 validation 3.7099650329962275\n",
      "epoch: 2640 test error: 3.872700857370575\n",
      "epoch:  2645 loss 3.0961688e-05 train 0.3939826043406219 validation 3.7115673129929316\n",
      "epoch: 2645 test error: 3.8730906508158527\n",
      "epoch:  2650 loss 1.1054536e-05 train 0.21201442061116108 validation 3.690343290599608\n",
      "epoch: 2650 test error: 3.850517459755185\n",
      "epoch:  2655 loss 8.152883e-06 train 0.15531138717341358 validation 3.666116963828255\n",
      "epoch: 2655 test error: 3.8344588068596868\n",
      "epoch:  2660 loss 1.1178744e-05 train 0.21777744742695845 validation 3.656763009683766\n",
      "epoch: 2660 test error: 3.830609231048902\n",
      "epoch:  2665 loss 1.1132185e-05 train 0.21766995449116527 validation 3.654548274454742\n",
      "epoch: 2665 test error: 3.830289291536338\n",
      "epoch:  2670 loss 8.545895e-06 train 0.1703970181794225 validation 3.65667128047147\n",
      "epoch: 2670 test error: 3.8310470560795737\n",
      "epoch:  2675 loss 7.4749446e-06 train 0.13482171994566533 validation 3.663989190343186\n",
      "epoch: 2675 test error: 3.833567044895322\n",
      "epoch:  2680 loss 7.9876e-06 train 0.1560744270774378 validation 3.670793985959559\n",
      "epoch: 2680 test error: 3.8368932696399844\n",
      "epoch:  2685 loss 7.74631e-06 train 0.15033570263118207 validation 3.673711475793509\n",
      "epoch: 2685 test error: 3.838896574465164\n",
      "epoch:  2690 loss 7.270866e-06 train 0.13146144024163606 validation 3.671086862701375\n",
      "epoch: 2690 test error: 3.838398756143259\n",
      "epoch:  2695 loss 7.3717038e-06 train 0.1407940296097894 validation 3.66802079908324\n",
      "epoch: 2695 test error: 3.8380782622577536\n",
      "epoch:  2700 loss 7.195352e-06 train 0.13379147734543115 validation 3.6691836285554165\n",
      "epoch: 2700 test error: 3.8393642067193965\n",
      "epoch:  2705 loss 7.1153668e-06 train 0.13250736634570973 validation 3.67213568268325\n",
      "epoch: 2705 test error: 3.8412272937194083\n",
      "epoch:  2710 loss 7.054839e-06 train 0.13135891637221062 validation 3.6741614551534347\n",
      "epoch: 2710 test error: 3.8430832557118717\n",
      "epoch:  2715 loss 6.9785133e-06 train 0.13042460612906745 validation 3.673266469407238\n",
      "epoch: 2715 test error: 3.843349800415752\n",
      "epoch:  2720 loss 6.9127636e-06 train 0.12887921174020397 validation 3.6742336070057418\n",
      "epoch: 2720 test error: 3.8443938033681855\n",
      "epoch:  2725 loss 6.8585155e-06 train 0.12876931234666072 validation 3.675734172533123\n",
      "epoch: 2725 test error: 3.846073719957678\n",
      "epoch:  2730 loss 6.7896162e-06 train 0.12759059214018614 validation 3.6764934117255472\n",
      "epoch: 2730 test error: 3.8469114054582914\n",
      "epoch:  2735 loss 6.7369665e-06 train 0.12679748247498143 validation 3.6767150032812146\n",
      "epoch: 2735 test error: 3.8477245964926645\n",
      "epoch:  2740 loss 6.6786665e-06 train 0.12595743190997874 validation 3.6782031486571793\n",
      "epoch: 2740 test error: 3.848993717236996\n",
      "epoch:  2745 loss 6.62261e-06 train 0.12527752383730464 validation 3.6781184971415013\n",
      "epoch: 2745 test error: 3.8498917509565658\n",
      "epoch:  2750 loss 6.5695294e-06 train 0.12464085744004184 validation 3.679868135416119\n",
      "epoch: 2750 test error: 3.85110709535823\n",
      "epoch:  2755 loss 6.523453e-06 train 0.1245234346028771 validation 3.679492857111847\n",
      "epoch: 2755 test error: 3.851583760247208\n",
      "epoch:  2760 loss 6.499378e-06 train 0.12581352499304907 validation 3.68244331798394\n",
      "epoch: 2760 test error: 3.8535034359191824\n",
      "epoch:  2765 loss 6.5718546e-06 train 0.13381180374181026 validation 3.678774941799407\n",
      "epoch: 2765 test error: 3.8529049578111563\n",
      "epoch:  2770 loss 7.187483e-06 train 0.15326059684338078 validation 3.690142636322159\n",
      "epoch: 2770 test error: 3.8584239493921384\n",
      "epoch:  2775 loss 1.1380742e-05 train 0.23036439605975398 validation 3.67009266830203\n",
      "epoch: 2775 test error: 3.8555859048045638\n",
      "epoch:  2780 loss 3.93056e-05 train 0.44773255564266057 validation 3.754355504885904\n",
      "epoch: 2780 test error: 3.919296039801962\n",
      "epoch:  2785 loss 0.00013314895 train 0.8215046042745295 validation 3.6854502141994008\n",
      "epoch: 2785 test error: 3.9464517223247926\n",
      "epoch:  2790 loss 1.626485e-05 train 0.2815007842856514 validation 3.681688768061308\n",
      "epoch: 2790 test error: 3.8517858519646992\n",
      "epoch:  2795 loss 4.9457798e-05 train 0.5054234330880234 validation 3.7240473828245286\n",
      "epoch: 2795 test error: 3.8901720091225833\n",
      "epoch:  2800 loss 1.6409673e-05 train 0.28431026140152554 validation 3.710447567537675\n",
      "epoch: 2800 test error: 3.8738144862536723\n",
      "epoch:  2805 loss 7.3219067e-06 train 0.16186181761392512 validation 3.67083581713846\n",
      "epoch: 2805 test error: 3.842743097146215\n",
      "epoch:  2810 loss 1.4143432e-05 train 0.2651428177904569 validation 3.652544059072056\n",
      "epoch: 2810 test error: 3.838602682420532\n",
      "epoch:  2815 loss 9.674706e-06 train 0.20752885980962996 validation 3.65293729139067\n",
      "epoch: 2815 test error: 3.839392255036581\n",
      "epoch:  2820 loss 6.0028565e-06 train 0.12053379994209419 validation 3.6618916623640474\n",
      "epoch: 2820 test error: 3.83858467362622\n",
      "epoch:  2825 loss 7.6229667e-06 train 0.16936494998471538 validation 3.6752427823116123\n",
      "epoch: 2825 test error: 3.844571578172709\n",
      "epoch:  2830 loss 6.6140324e-06 train 0.14626930088874138 validation 3.678344182437348\n",
      "epoch: 2830 test error: 3.8465706835589306\n",
      "epoch:  2835 loss 5.931965e-06 train 0.1238238321239759 validation 3.671161559555698\n",
      "epoch: 2835 test error: 3.8441406841693606\n",
      "epoch:  2840 loss 6.2742015e-06 train 0.1399610149977012 validation 3.6679735430659046\n",
      "epoch: 2840 test error: 3.843509875084226\n",
      "epoch:  2845 loss 5.7223115e-06 train 0.11608061890338933 validation 3.671832508975491\n",
      "epoch: 2845 test error: 3.845712983808623\n",
      "epoch:  2850 loss 5.8704886e-06 train 0.12646358966702945 validation 3.676441978209754\n",
      "epoch: 2850 test error: 3.8481897617290444\n",
      "epoch:  2855 loss 5.6184376e-06 train 0.11501233739481201 validation 3.6750006163039797\n",
      "epoch: 2855 test error: 3.8483796787698745\n",
      "epoch:  2860 loss 5.642097e-06 train 0.1200446857395609 validation 3.6738034796783143\n",
      "epoch: 2860 test error: 3.848429753339208\n",
      "epoch:  2865 loss 5.5489536e-06 train 0.11614909951926677 validation 3.677741243158053\n",
      "epoch: 2865 test error: 3.850896807493001\n",
      "epoch:  2870 loss 5.4649377e-06 train 0.11270925650348429 validation 3.6777462778194523\n",
      "epoch: 2870 test error: 3.8516632305534855\n",
      "epoch:  2875 loss 5.42966e-06 train 0.11365836785125552 validation 3.6774221872806367\n",
      "epoch: 2875 test error: 3.852070642000559\n",
      "epoch:  2880 loss 5.3968442e-06 train 0.11339181886057326 validation 3.6808846371586164\n",
      "epoch: 2880 test error: 3.854348362159359\n",
      "epoch:  2885 loss 5.3740937e-06 train 0.11581726354057369 validation 3.6781882757652555\n",
      "epoch: 2885 test error: 3.854114040901164\n",
      "epoch:  2890 loss 5.365696e-06 train 0.11624917489923225 validation 3.6832102821255477\n",
      "epoch: 2890 test error: 3.8565702102627393\n",
      "epoch:  2895 loss 5.45778e-06 train 0.12449164121864523 validation 3.678013210904729\n",
      "epoch: 2895 test error: 3.8554459822193223\n",
      "epoch:  2900 loss 6.0089524e-06 train 0.141900385883936 validation 3.6902085269119835\n",
      "epoch: 2900 test error: 3.8612931735546394\n",
      "epoch:  2905 loss 8.877249e-06 train 0.2024905349235228 validation 3.670152370344766\n",
      "epoch: 2905 test error: 3.857132672380351\n",
      "epoch:  2910 loss 2.3094475e-05 train 0.3456753528643401 validation 3.732358238202717\n",
      "epoch: 2910 test error: 3.8958946803147034\n",
      "epoch:  2915 loss 7.8479294e-05 train 0.6427429475488176 validation 3.6702576193337673\n",
      "epoch: 2915 test error: 3.9176296156041284\n",
      "epoch:  2920 loss 8.385628e-05 train 0.6682570973722424 validation 3.766926475128283\n",
      "epoch: 2920 test error: 3.9293739761506665\n",
      "epoch:  2925 loss 1.1452708e-05 train 0.2343942627352308 validation 3.718020655521914\n",
      "epoch: 2925 test error: 3.88403916744935\n",
      "epoch:  2930 loss 2.4458155e-05 train 0.36292348103248107 validation 3.6548443968111055\n",
      "epoch: 2930 test error: 3.8508886579423023\n",
      "epoch:  2935 loss 1.4050403e-05 train 0.2697086438079746 validation 3.6542427957068027\n",
      "epoch: 2935 test error: 3.8510190908420823\n",
      "epoch:  2940 loss 7.3311385e-06 train 0.1745723194161305 validation 3.67104964610247\n",
      "epoch: 2940 test error: 3.8481029477904034\n",
      "epoch:  2945 loss 1.0542562e-05 train 0.22415540768146305 validation 3.6907222708167273\n",
      "epoch: 2945 test error: 3.857819329832102\n",
      "epoch:  2950 loss 4.9401083e-06 train 0.1109125673075166 validation 3.6738115747454323\n",
      "epoch: 2950 test error: 3.8484009555300727\n",
      "epoch:  2955 loss 7.2447056e-06 train 0.1775993717903838 validation 3.6627240737074875\n",
      "epoch: 2955 test error: 3.8451631806436097\n",
      "epoch:  2960 loss 4.7929925e-06 train 0.10675292508918818 validation 3.6720962911339177\n",
      "epoch: 2960 test error: 3.8483279818848484\n",
      "epoch:  2965 loss 5.695511e-06 train 0.1410227570674003 validation 3.6830353513229057\n",
      "epoch: 2965 test error: 3.853884020696993\n",
      "epoch:  2970 loss 4.8189927e-06 train 0.1136478887691827 validation 3.6747979609431822\n",
      "epoch: 2970 test error: 3.850793980264633\n",
      "epoch:  2975 loss 4.845147e-06 train 0.11643279474324836 validation 3.67313861919128\n",
      "epoch: 2975 test error: 3.8509394041479914\n",
      "epoch:  2980 loss 4.91819e-06 train 0.11923700046136582 validation 3.6814788052684833\n",
      "epoch: 2980 test error: 3.854939421669728\n",
      "epoch:  2985 loss 4.619709e-06 train 0.10799777930196794 validation 3.6764659734985994\n",
      "epoch: 2985 test error: 3.853418368201822\n",
      "epoch:  2990 loss 4.5157685e-06 train 0.10187575433302994 validation 3.67828280946361\n",
      "epoch: 2990 test error: 3.8547783690497335\n",
      "epoch:  2995 loss 4.5225725e-06 train 0.10476812201864377 validation 3.6819112246171724\n",
      "epoch: 2995 test error: 3.857067435942749\n"
     ]
    }
   ],
   "source": [
    "optimizer_per = tf.keras.optimizers.Adam(learning_rate=.005)\n",
    "for epoch in range(3000):\n",
    "    j = 0\n",
    "    mini_batch_y = np.zeros((train_index.shape[0], 56, 56, 6))\n",
    "    mini_batch_cbcr = np.zeros((train_index.shape[0], 28, 28, 6))\n",
    "    for i in train_index:\n",
    "        jpeg_file = '/home/ldu/CL_gaze_project/Personalize/subject0102/' + str(i) + '.jpg'\n",
    "        dct_y, dct_cb, dct_cr = load(jpeg_file)\n",
    "        # channel selection\n",
    "        dct_y = np.concatenate((np.concatenate((dct_y[:, :, 0:3], dct_y[:, :, 8:10]), axis=2),\n",
    "                                np.reshape(dct_y[:, :, 16], (56, 56, 1))), axis=2)\n",
    "\n",
    "        dct_cb = np.concatenate((dct_cb[:, :, 0:2], np.reshape(dct_cb[:, :, 8], (28, 28, 1))), axis=2)\n",
    "        dct_cr = np.concatenate((dct_cr[:, :, 0:2], np.reshape(dct_cr[:, :, 8], (28, 28, 1))), axis=2)\n",
    "        cb_cr = np.concatenate([dct_cb, dct_cr], 2)\n",
    "        #cb_cr = np.repeat(np.repeat(cb_cr, 2, 0), 2, 1)\n",
    "        #img = cv2.resize(img, (224,224))\n",
    "        mini_batch_y[j, :, :, :] = dct_y\n",
    "        mini_batch_cbcr[j, :, :, :] = cb_cr\n",
    "        j += 1\n",
    "\n",
    "    # optimization\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = GazeEstimation(feature_extractor([mini_batch_y, mini_batch_cbcr]))\n",
    "        loss = tf.reduce_mean(tf.square(labels_per[train_index] - prediction))\n",
    "        #GazeEstimation.save_weights(\"/home/ldu/CL_gaze_project/dct_mix_late_concat_new_structure/Gaze_2\")\n",
    "        #print(loss.numpy())\n",
    "\n",
    "    grads = tape.gradient(loss, GazeEstimation.trainable_variables)\n",
    "    optimizer_per.apply_gradients(grads_and_vars=zip(grads, GazeEstimation.trainable_variables))\n",
    "\n",
    "    # training error\n",
    "\n",
    "    train_angle_error = common_functions.avg_angle_error(prediction, labels_per[train_index])\n",
    "    train_error_per.append(train_angle_error.numpy())\n",
    "\n",
    "    # validation error\n",
    "    if epoch % 5 == 0:\n",
    "        j = 0\n",
    "        mini_batch_y = np.zeros((val_index.shape[0], 56, 56, 6))\n",
    "        mini_batch_cbcr = np.zeros((val_index.shape[0], 28, 28, 6))\n",
    "        for i in val_index:\n",
    "            jpeg_file = '/home/ldu/CL_gaze_project/Personalize/subject0102/' + str(i) + '.jpg'\n",
    "            dct_y, dct_cb, dct_cr = load(jpeg_file)\n",
    "            # channel selection\n",
    "            dct_y = np.concatenate((np.concatenate((dct_y[:, :, 0:3], dct_y[:, :, 8:10]), axis=2),\n",
    "                                    np.reshape(dct_y[:, :, 16], (56, 56, 1))), axis=2)\n",
    "\n",
    "            dct_cb = np.concatenate((dct_cb[:, :, 0:2], np.reshape(dct_cb[:, :, 8], (28, 28, 1))), axis=2)\n",
    "            dct_cr = np.concatenate((dct_cr[:, :, 0:2], np.reshape(dct_cr[:, :, 8], (28, 28, 1))), axis=2)\n",
    "            cb_cr = np.concatenate([dct_cb, dct_cr], 2)\n",
    "            #cb_cr = np.repeat(np.repeat(cb_cr, 2, 0), 2, 1)\n",
    "            #img = cv2.resize(img, (224,224))\n",
    "            mini_batch_y[j, :, :, :] = dct_y\n",
    "            mini_batch_cbcr[j, :, :, :] = cb_cr\n",
    "            j += 1\n",
    "        prediction = GazeEstimation(feature_extractor([mini_batch_y, mini_batch_cbcr]))\n",
    "        validation_angle_error = common_functions.avg_angle_error(prediction, labels_per[val_index])\n",
    "\n",
    "        print(\"epoch: \", epoch, \"loss\", loss.numpy(), \"train\", train_angle_error.numpy(),\n",
    "              \"validation\", validation_angle_error.numpy())\n",
    "        val_error_per.append(validation_angle_error.numpy())\n",
    "    # test\n",
    "    if epoch % 5 == 0:\n",
    "        batch_number = 5\n",
    "        test_error = 0.\n",
    "        for batch_i in range(batch_number):\n",
    "            start = batch_i * 100\n",
    "            end = (batch_i + 1) * 100\n",
    "            if end >= test_index.shape[0]:\n",
    "                end = test_index.shape[0]\n",
    "            test_batch_y = np.zeros((test_index[start: end].shape[0], 56, 56, 6))\n",
    "            test_batch_cbcr = np.zeros((test_index[start: end].shape[0], 28, 28, 6))\n",
    "            j = 0\n",
    "            for i in test_index[start: end]:\n",
    "                jpeg_file = '/home/ldu/CL_gaze_project/Personalize/subject0102/' + str(i) + '.jpg'\n",
    "                dct_y, dct_cb, dct_cr = load(jpeg_file)\n",
    "                # channel selection\n",
    "                dct_y = np.concatenate((np.concatenate((dct_y[:, :, 0:3], dct_y[:, :, 8:10]), axis=2),\n",
    "                                        np.reshape(dct_y[:, :, 16], (56, 56, 1))), axis=2)\n",
    "\n",
    "                dct_cb = np.concatenate((dct_cb[:, :, 0:2], np.reshape(dct_cb[:, :, 8], (28, 28, 1))), axis=2)\n",
    "                dct_cr = np.concatenate((dct_cr[:, :, 0:2], np.reshape(dct_cr[:, :, 8], (28, 28, 1))), axis=2)\n",
    "                cb_cr = np.concatenate([dct_cb, dct_cr], 2)\n",
    "                #cb_cr = np.repeat(np.repeat(cb_cr, 2, 0), 2, 1)\n",
    "                #img = cv2.resize(img, (224,224))\n",
    "                test_batch_y[j, :, :, :] = dct_y\n",
    "                test_batch_cbcr[j, :, :, :] = cb_cr\n",
    "                j += 1\n",
    "            prediction = GazeEstimation(feature_extractor([test_batch_y, test_batch_cbcr]))\n",
    "            test_error += common_functions.avg_angle_error(prediction, labels_per[test_index[start: end]]) \\\n",
    "                          * test_index[start: end].shape[0]\n",
    "        test_error_per.append(test_error.numpy() / test_index.shape[0])\n",
    "        print(\"epoch:\", epoch, \"test error:\", test_error.numpy() / test_index.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7083d0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing time 0.01410609444603324\n"
     ]
    }
   ],
   "source": [
    "# test inference time\n",
    "import time\n",
    "test_y = np.zeros((1, 56, 56, 6))\n",
    "test_cbcr = np.zeros((1, 28, 28, 6))\n",
    "jpeg_file = '/home/ldu/CLAE_Advdrop/Personalize/subject0102/1.jpg'\n",
    "t0 = time.perf_counter()\n",
    "for _ in range(100):\n",
    "    dct_y, dct_cb, dct_cr = load(jpeg_file)\n",
    "        # channel selection\n",
    "    dct_y = np.concatenate((np.concatenate((dct_y[:, :, 0:3], dct_y[:, :, 8:10]), axis=2),\n",
    "                            np.reshape(dct_y[:, :, 16], (56, 56, 1))), axis=2)\n",
    "\n",
    "    dct_cb = np.concatenate((dct_cb[:, :, 0:2], np.reshape(dct_cb[:, :, 8], (28, 28, 1))), axis=2)\n",
    "    dct_cr = np.concatenate((dct_cr[:, :, 0:2], np.reshape(dct_cr[:, :, 8], (28, 28, 1))), axis=2)\n",
    "    cb_cr = np.concatenate([dct_cb, dct_cr], 2)\n",
    "    test_y[0, :, :, :] = dct_y\n",
    "    test_cbcr[0, :, :, :] = cb_cr\n",
    "    pre = GazeEstimation(feature_extractor([test_y, test_cbcr]))\n",
    "t1 = time.perf_counter()\n",
    "print('processing time',(t1-t0)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee5ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0.299, 0.587, 0.114], [-0.169, -0.331, 0.5], [0.5, -0.419, -0.081]])\n",
    "\n",
    "\n",
    "def calculate_dct_coeff(img_local):\n",
    "\n",
    "    img_local.astype(float)\n",
    "    row_after_dct = int(img_local.shape[0] / 8)\n",
    "    col_after_dct = int(img_local.shape[1] / 8)\n",
    "    dct_coefficients = np.zeros((row_after_dct, col_after_dct, 64))\n",
    "\n",
    "    # calculate dct coefficients for each block\n",
    "    for i in range(row_after_dct):\n",
    "        for j in range(col_after_dct):\n",
    "            dct_block = cv2.dct(img_local[i * 8:(i + 1) * 8, j * 8:(j + 1) * 8])\n",
    "            dct_block = np.reshape(dct_block, (64))\n",
    "            dct_coefficients[i, j, :] = dct_block\n",
    "\n",
    "    return np.round(dct_coefficients).astype(int)\n",
    "\n",
    "\n",
    "def RGB_to_dct_coeff(rgb_img):\n",
    "\n",
    "    rgb_img.astype(float)\n",
    "    y = np.sum(rgb_img * A[0, :], 2) - 128\n",
    "    cb = np.sum(rgb_img * A[1, :], 2)\n",
    "    cr = np.sum(rgb_img * A[2, :], 2)\n",
    "    cb = cb[::2, ::2]\n",
    "    cr = cr[::2, ::2]\n",
    "    y_coeff = calculate_dct_coeff(y)\n",
    "    cb_coeff = calculate_dct_coeff(cb)\n",
    "    cr_coeff = calculate_dct_coeff(cr)\n",
    "\n",
    "    return y_coeff, cb_coeff, cr_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aef14c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
